{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "uFWJgHmZUIwA"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/main/03-Deep-Learning/notebooks/01-Propagacion-directa.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT157NOXUIwE"
      },
      "source": [
        "<h1 > Redes Neuronales Artificiales - Propagación directa </h1>    \n",
        "\n",
        "El objetivo de esta notebook es implementar la propagación directa de una red neuronal para realizar predicciones en la tarea de clasificación de dígitos escritos a mano del dataset MNIST.\n",
        "\n",
        "Para esto, usaremos matrices de pesos precalculados y evaluaremos la tarea de clasificación usando el conjunto de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iiYjboDVUIwH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sólo necesitamos el conjunto de prueba del dataset MNIST. **Estamos en el escenario de que el modelo ya está entrenado.**"
      ],
      "metadata": {
        "id": "v65ibZs0cK0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "_, (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "VBDNhkICXsJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos la carpeta *data* del repositorio de Github"
      ],
      "metadata": {
        "id": "StHXfTYmcQHq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVHxdJnbUIwL"
      },
      "outputs": [],
      "source": [
        "!apt-get -qq install > /dev/null subversion\n",
        "\n",
        "!svn checkout \"https://github.com/DCDPUAEM/DCDP/trunk/03-Deep-Learning/data/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YWSMkxUUIwM"
      },
      "source": [
        "## Clasificación multiclase\n",
        "\n",
        "Para este ejercicio, usaremos una red neuronal para reconocer los dígitos escritos a mano (de 0 a 9). \n",
        "\n",
        "### Conjunto de datos\n",
        "\n",
        "* Usaremos el conjunto de prueba del dataset MNIST de keras, el cual que contiene 10000 ejemplos de prueba de dígitos escritos a mano (este es un subconjunto del conjunto de datos de dígitos escritos a mano [MNIST] (http://yann.lecun.com/exdb/mnist). \n",
        "\n",
        "* Cada ejemplo de prueba es una imagen del dígito en escala de grises de $28\\times 28$ píxeles. Cada píxel está representado por un número real que indica la intensidad de la escala de grises en esa ubicación. \n",
        "\n",
        "* La cuadrícula de pixeles de $28\\times 28$ se *desenrolla* en un vector de 784 dimensiones. Cada uno de estos ejemplos se convierte en una sola fila en nuestra matriz de datos `X`. Esto nos da una matriz $X$ de $10000\\times 784$ donde cada fila es una instancia para el clasificador.\n",
        "\n",
        "$$ X = \\begin{bmatrix} - \\: (x ^ {(1)})^T \\: - \\\\ - \\: (x ^{(2)})^ T \\: - \\\\ \\vdots \\\\ - \\: (x ^ {(m)})^ T \\: - \\end{bmatrix} $$\n",
        "\n",
        "* Los pesos de la red fueron entrenados en una notebook que usaremos después. Los pesos están almacenados como arreglos de numpy en formato *npy*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIIvpLmmUIwN"
      },
      "outputs": [],
      "source": [
        "X = X_test.copy()\n",
        "y = y_test.copy() \n",
        "\n",
        "m = y.shape[0] # Número de instancias\n",
        "\n",
        "indices = np.random.permutation(m)\n",
        "\n",
        "# ----- Elegimos 100 puntos al azar para desplegar\n",
        "rand_indices = np.random.choice(m, 100, replace=False)\n",
        "sel = X[rand_indices, :].reshape((10,10,-1))\n",
        "\n",
        "# ----- Visualizamos de los datos\n",
        "fig, axarr = plt.subplots(10,10,figsize=(10,10))\n",
        "for i in range(10):\n",
        "    for j in range(10):\n",
        "        axarr[i,j].imshow(sel[i,j].reshape((28,28)),cmap='plasma')          \n",
        "        axarr[i,j].axis('off')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyogHyZXUIwS"
      },
      "source": [
        "## Modelo de red neuronal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSvnHbJYUIwT"
      },
      "source": [
        "La red neuronal que vamos a implementar es la siguiente:\n",
        "\n",
        "<img align=\"center\" width=\"50%\" src=\"https://github.com/DCDPUAEM/DCDP/blob/main/03-Deep-Learning/img/redneuronal.png?raw=1\"/> \n",
        "\n",
        "Tiene 3 capas: una capa de entrada, una capa oculta y una capa de salida. Recuerda que nuestras entradas son valores de pixeles de imágenes de dígitos. Dado que las imágenes tienen un tamaño de 28 × 28, esto nos da 784 neuronas de capa de entrada (excluyendo la neurona de bias adicional que siempre agrega una entrada). \n",
        "\n",
        "Se te ha proporcionado un conjunto de parámetros de la red ($\\Theta^{(1)}$, $\\Theta^{(2)}$) ya entrenados (fueron generados con el modelo de la siguiente notebook). La siguiente celda carga esos parámetros en `Theta1` y` Theta2`. Los parámetros tienen dimensiones para una red neuronal con 8 neuronas en la segunda capa y 10 neuronas de salida (correspondientes a las clases de cada uno de los 10 dígitos)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aJuqbgOUIwU"
      },
      "source": [
        "### Parámetros para el ejercicio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos los pesos a partir de arreglos de numpy guardados"
      ],
      "metadata": {
        "id": "z3NbACoPeZC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights_1 = np.load('/content/data/mnist_weights1.npy')\n",
        "weights_2 = np.load('/content/data/mnist_weights2.npy')\n",
        "\n",
        "biases_1 = np.load('/content/data/mnist_biases1.npy')\n",
        "biases_2 = np.load('/content/data/mnist_biases2.npy')"
      ],
      "metadata": {
        "id": "rFnbWukLYrZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(weights_1.shape)\n",
        "print(biases_1.shape)\n",
        "print(weights_2.shape)\n",
        "print(biases_2.shape)"
      ],
      "metadata": {
        "id": "YvKG0KNUgfzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Juntamos los biases \"encima\" de los pesos:\n",
        "\n",
        "$$b=(b_1,...,b_{8})$$\n",
        "\n",
        "$$W = \\left(\\begin{array}{ccc}\n",
        "w_{1,1} & ... & w_{1,8} \\\\\n",
        "... & ... & ... \\\\\n",
        "w_{784,1} & ... & w_{784,8}\n",
        "\\end{array}\\right)$$\n",
        "\n",
        "para obtener:\n",
        "\n",
        "$$\\Theta_1 = \\left(\\begin{array}{ccc}\n",
        "b_1 & ... & b_8 \\\\\n",
        "w_{1,1} & ... & w_{1,8} \\\\\n",
        "... & ... & ... \\\\\n",
        "w_{784,1} & ... & w_{784,8}\n",
        "\\end{array}\\right)$$\n",
        "\n",
        "al final, transponemos y tenemos $\\Theta_1^T$."
      ],
      "metadata": {
        "id": "xk_lCSmmgM6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Theta1 = np.transpose(np.vstack((biases_1,weights_1)))\n",
        "print(Theta1.shape)\n",
        "\n",
        "Theta2 = np.transpose(np.vstack((biases_2,weights_2)))\n",
        "print(Theta2.shape)"
      ],
      "metadata": {
        "id": "iwRYTlolZsOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZkGIWXZUIwV"
      },
      "outputs": [],
      "source": [
        "# Parámetros\n",
        "neuronas_de_entrada  = 784 \n",
        "neuronas_ocultas = 8  \n",
        "etiquetas = 10          # 10 etiquetas, clases de 0 a 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5sUpvE8UIwW"
      },
      "outputs": [],
      "source": [
        "print(Theta1.shape)\n",
        "print(Theta2.shape)\n",
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOJeotBpUIwX"
      },
      "source": [
        "## Propagación directa y predicción"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipO6NT4pUIwX"
      },
      "source": [
        "* Completa el código en la función predicción para devolver la predicción de la red neuronal. \n",
        "* Debes implementar el cálculo directo de $h_{\\theta}(x^{(i)})$ para cada ejemplo $i$ y devolver las predicciones asociadas. \n",
        "* De acuerdo con la estrategia de clasificación de uno contra todos, la predicción de la red neuronal será la etiqueta que tenga el mayor valor $(h_{\\theta}(x^(i)))_k$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJu7coyjUIwY"
      },
      "source": [
        "### Funciones de activación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNCbe3XWUIwY"
      },
      "outputs": [],
      "source": [
        "def sigmoide(z):\n",
        "    return 1.0 / (1.0 + np.exp(-z))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm3PKbktUIwZ"
      },
      "source": [
        "### ⭕ Implementa la función `predict`\n",
        "\n",
        "\n",
        "**Nota de implementación:** La matriz $X$ contiene las instancias en cada fila. Cuando completes el código en la función $predict$, deberás agregar la columna de 1's a la matriz. Las matrices $\\text{Theta}_1$ y $\\text{Theta}_2$ contienen los parámetros para cada neurona en sus filas. Específicamente, la primera fila de $Theta1$ corresponde a los datos que van a ser procesados por la primera neurona oculta en la segunda capa. En `numpy`, cuando se calcula $z^{(2)}=\\Theta^{(1)}a^{(1)}$, hay que asegurarse de indexar (y si es necesario, transponer) $X$ correctamente para obtener $a^{(l)}$ como un vector 1-dimensional.\n",
        "\n",
        "El vector que debes obtener en la capa de salida es de $10000\\times 10$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eo2_Nbw-UIwZ"
      },
      "outputs": [],
      "source": [
        "def predict(Theta1, Theta2, X):\n",
        "    \"\"\"\n",
        "    Predice la etiqueta de una entrada dada una red entrenada.\n",
        "    \n",
        "    Entradas:\n",
        "    ----------\n",
        "    Theta1 : tipo arreglo \n",
        "        Pesos para la capa 1 de la red neuronal.\n",
        "        Tiene shape (2da capa x capa de entrada)\n",
        "    \n",
        "    Theta2: tipo arreglo\n",
        "        Pesos para la capa 2 de la red neuronal.\n",
        "        Tiene shape (capa de salida x 2da capa)\n",
        "    \n",
        "    X : tipo arreglo (matriz)\n",
        "        Las imágenes de entrada de tamaño (instancias x dimensiones de la imagen).\n",
        "    \n",
        "    Salidas: \n",
        "    ------\n",
        "    p : tipo arreglo\n",
        "        Vector de predicciones con etiquetas predichas para cada ejemplo\n",
        "        Su tamaño es igual al número de instancias.\n",
        "    \n",
        "    \"\"\"\n",
        "    # Necesitamos a X 2D\n",
        "    print(X.shape)\n",
        "    if X.ndim == 1:\n",
        "        X = X[None]\n",
        "    \n",
        "    # numero de instancias y etiquetas\n",
        "    m = X.shape[0]\n",
        "    etiquetas = Theta2.shape[0]\n",
        "\n",
        "    # Cambiamos a coordenadas homogeneas las entradas de la matriz X\n",
        "    ones = np.ones((m,1))\n",
        "    a1 = np.hstack((ones,X))\n",
        "\n",
        "    \"\"\" ====================== TU CODIGO AQUI ======================\"\"\"\n",
        "\n",
        "    \"\"\" =============================================================\"\"\"\n",
        "    p = np.argmax(h,axis=1) \n",
        "    return p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGjyGLo8UIwa"
      },
      "source": [
        "### Prueba el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplanamos cada instancia como un vector de $28\\times 28=784$ componentes."
      ],
      "metadata": {
        "id": "luA1fMwOE6mk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_flatten = X.reshape((X.shape[0],X.shape[1]*X.shape[1]))\n",
        "X_flatten.shape"
      ],
      "metadata": {
        "id": "1Q1HsoR4bSKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizamos las predicciones. Deberías ver una exactitud de aproximadamente 57%."
      ],
      "metadata": {
        "id": "tIUr8REQE5T5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoChl4B2UIwa"
      },
      "outputs": [],
      "source": [
        "pred = predict(Theta1, Theta2, X_flatten)\n",
        "print(f\"Accuracy: {np.mean(pred == y) * 100} %\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred.shape"
      ],
      "metadata": {
        "id": "aMxZGNbWB-nF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej75C1s_UIwa"
      },
      "source": [
        "### Para visualizar una por una las instancias de entrada y la predicción correspondiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOin2dMtUIwb"
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots(figsize=(4,4))\n",
        "if indices.size > 0:\n",
        "    i, indices = indices[0], indices[1:]\n",
        "    ax.imshow(X[i,:].reshape((28,28)))          \n",
        "    ax.axis('off')    \n",
        "    pred = predict(Theta1, Theta2, X_flatten[i, :])\n",
        "    print(f'Predición de la Red Neuronal: {pred}')\n",
        "else:\n",
        "    print('No hay más imágenes.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##¿Qué pasa si usamos una matriz de pesos sin entrenar?"
      ],
      "metadata": {
        "id": "clezAOX6mruE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos matrices aleatorias de pesos, de la misma forma que las matrices de pesos que usamos."
      ],
      "metadata": {
        "id": "HAi82JiwodF-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0Hd4B9XUIwb"
      },
      "outputs": [],
      "source": [
        "weights1_shape = Theta1.shape\n",
        "weights2_shape = Theta2.shape\n",
        "\n",
        "random_weights1 = np.random.randn(*weights1_shape)\n",
        "random_weights2 = np.random.randn(*weights2_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculamos el accuracy"
      ],
      "metadata": {
        "id": "Wnso7_Uoot2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = predict(random_weights1, random_weights2, X_flatten)\n",
        "print(f\"Accuracy: {np.mean(pred == y) * 100} %\")"
      ],
      "metadata": {
        "id": "MFbNeowroU75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observamos algunos ejemplos de predicciones"
      ],
      "metadata": {
        "id": "FVo9FW5wqL3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------ Obtenemos algunos índices aleatorios:\n",
        "some_idxs = np.random.choice(list(range(y.shape[0])),size=6,replace=False)\n",
        "\n",
        "fig, axes = plt.subplots(ncols=6, sharex=False,\n",
        "\t\t\t sharey=True, figsize=(10, 4))\n",
        "for i,idx in enumerate(some_idxs):\n",
        "\taxes[i].set_title(pred[idx],fontsize=15)\n",
        "\taxes[i].imshow(X[idx], cmap='gray')\n",
        "\taxes[i].get_xaxis().set_visible(False)\n",
        "\taxes[i].get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t-l4jFccoXHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BK5m8VAXqXND"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}