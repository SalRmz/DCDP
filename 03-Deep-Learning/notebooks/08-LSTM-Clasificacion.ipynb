{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/main/03-Deep-Learning/notebooks/08-LSTM-Clasificacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "HSyYPlAKVJGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Long Short-Term Memory Networks: Sentiment Analysis</h1>\n",
        "\n",
        "En esta notebook usaremos redes LSTM para análisis de sentimientos. Estudiaremos el dataset de reviews de películas de IMDB. Este es un dataset muy usado para tareas de análisis de sentimientos.\n",
        "\n",
        "En esta tarea no nos interesa tener una salida en cada elemento de la secuencia, solamente queremos la salida al final de la secuencia.\n",
        "\n",
        "\n",
        "<img align=\"left\" width=\"50%\" src=\"../img/LSTM.png\"/>"
      ],
      "metadata": {
        "id": "0-QiMUBj-xrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print('GPU presente en: {}'.format(tf.test.gpu_device_name()))"
      ],
      "metadata": {
        "id": "WtXXpPwLBFOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QsQlfmy8YYDK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import keras\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "import math\n",
        "import nltk"
      ],
      "metadata": {
        "id": "8cOV9jZib6RU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM for Sentiment Analysis"
      ],
      "metadata": {
        "id": "cCAbMc5gD32H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## El conjunto de datos"
      ],
      "metadata": {
        "id": "Tje27BV-GVdi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMDB\n",
        "\n",
        "* Original source: http://ai.stanford.edu/~amaas/data/sentiment/\n",
        "* Kaggle: https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
      ],
      "metadata": {
        "id": "xmH2IBtuGXaQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bajamos este archivo desde Drive. En caso de tener problemas con el siguiente comando, puedes bajar el archivo de [aquí](https://drive.google.com/uc?id=1TewLD3BbgqV1t2I905Al3vm_VqUzoPzg) y luego subirlo manualmente a Colab."
      ],
      "metadata": {
        "id": "wmtR29HCIDIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gdown"
      ],
      "metadata": {
        "id": "cs_gYXs9bSUb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1TewLD3BbgqV1t2I905Al3vm_VqUzoPzg"
      ],
      "metadata": {
        "id": "QscswTFwBSQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leemos el dataframe"
      ],
      "metadata": {
        "id": "P5-xL-av1NaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/IMDB Dataset.csv')\n",
        "display(df)"
      ],
      "metadata": {
        "id": "l4E4bIiAZuUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tenemos dos clases. Es un problema de clasificación binaria"
      ],
      "metadata": {
        "id": "CD4QuvXuiteD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tomZPOEIijlR",
        "outputId": "57830578-def9-43be-e45b-2b9c13c61f5d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['positive', 'negative'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las clases están balanceadas"
      ],
      "metadata": {
        "id": "fxBUbJTYkbrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "sns.histplot(df['sentiment'].values)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cpKZ7laCj-4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "labels = df['sentiment'].values\n",
        "encoder = LabelEncoder()\n",
        "encoded_labels = encoder.fit_transform(labels)"
      ],
      "metadata": {
        "id": "MHnnTxIBqTrr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Before encoding {labels[:3]}\")\n",
        "print(f\"After encoding {encoded_labels[:3]}\")"
      ],
      "metadata": {
        "id": "Xsxqt7-75H21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limpiar texto"
      ],
      "metadata": {
        "id": "-7oXHUhEc2VU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "lKRYt1e9c2Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Limpiamos quitando stopwords, símbolos, tags HTML, etc usando herramientas del módulo `nltk`. Para hacer la limpieza también usamos expresiones regulares *regex*. Puedes practicar el uso de expresiones regulares [aquí](https://regex101.com/)."
      ],
      "metadata": {
        "id": "MIdZY2ozl6su"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "CLEANR = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
        "\n",
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def clean(text):\n",
        "    clean_text = re.sub(CLEANR, '', text.lower()) # Quitamos etiquetas HTML\n",
        "    clean_text = re.sub(r'[^\\w\\s]', '', clean_text.lower()) # Quitamos signos de puntuación y símbolos\n",
        "    clean_text = re.sub('[0-9]', '', clean_text.lower())  # Quitamos números\n",
        "    SW = stopwords.words('english') # Leemos la lista de stopwords del inglés\n",
        "    tokens_no_sw = [word for word in word_tokenize(clean_text) if not word in SW] # Quitamos stopwords\n",
        "    stems = \"\"\n",
        "    for w in tokens_no_sw:\n",
        "        stems += lemmatizer.lemmatize(w) + \" \"\n",
        "    return stems"
      ],
      "metadata": {
        "id": "M_P3nmmIi7PU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['clean'] = df['review'].apply(clean)\n",
        "df"
      ],
      "metadata": {
        "id": "v3gnQqwbnAt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = df['clean'].values"
      ],
      "metadata": {
        "id": "OB0IMAIwr0Cl"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separamos en entrenamiento y prueba"
      ],
      "metadata": {
        "id": "PM7lVVK1rwKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_reviews, test_reviews, train_labels, test_labels = train_test_split(reviews, encoded_labels, train_size=0.8)#, stratify = encoded_labels)}}\n",
        "\n",
        "print(f\"Shape of X_train: {train_reviews.shape}\")\n",
        "print(f\"Shape of X_test: {test_reviews.shape}\")"
      ],
      "metadata": {
        "id": "5DLD8mHjrzDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparamos los reviews. Este proceso consta de dos partes:\n",
        "\n",
        "1. Vectorización: La clase `Tokenizer` de `keras` permite vectorizar un corpus de textos, convirtiendo cada texto en una secuencia de índices (cada índice representa un token en un diccionario, los índices son $1,...,n$). No se toman en cuenta todas las palabras del vocabulario, se toman solamente las `vocab_size` más frecuentes.\n",
        "\n",
        "2. Padding: Las secuencias de índices tienen diferentes longitudes, dependiendo de la longitud del review. Las hacemos todas del mismo tamaño de acuerdo a dos criterios:\n",
        "\n",
        "    * Si la secuencia es más corta que el tamaño especificado, añadimos ceros al final de la secuencia.\n",
        "    * Si la secuencia es más larga que el tamaño especificado, truncamos la secuencia.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Importante**: Observa que el tokenizador se entrena con los textos de entrenamiento solamente. Después las secuencias de prueba se generan con este tokenizador.\n",
        "\n",
        "⭕ ¿Qué consecuencias tiene esto?\n",
        "\n"
      ],
      "metadata": {
        "id": "80DJ3ghFsibk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# ----- Hiperparámetros para este preprocesamiento\n",
        "vocab_size = 3000   # Nos limitaremos a ese número de palabras del vocabulario\n",
        "oov_tok = ''        # Las palabras fuera del vocabulario se reemplazarán con este string\n",
        "max_length = 200    # La longitud común deseada para las secuencias al hacer el padding\n",
        "\n",
        "#  ----- Entrenamos el tokenizador\n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(train_reviews)\n",
        "\n",
        "#  ----- Creamos las secuencias de entrenamiento y hacemos el padding\n",
        "train_sequences = tokenizer.texts_to_sequences(train_reviews)\n",
        "train_padded = pad_sequences(train_sequences, padding='post', maxlen=max_length)\n",
        "\n",
        "#  ----- Creamos las secuencias de prueba y hacemos el padding\n",
        "test_sequences = tokenizer.texts_to_sequences(test_reviews)\n",
        "test_padded = pad_sequences(test_sequences, padding='post', maxlen=max_length)"
      ],
      "metadata": {
        "id": "dtqOEb6Ksh93"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos un ejemplo de cómo se ven las secuencias. Observa, por ejemplo, la palabra *secret*."
      ],
      "metadata": {
        "id": "ldJbIKXBNZa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"NL review:\\n{train_reviews[0]}\\n\")\n",
        "print(f\"Sequence:\\n{train_sequences[0]}\\n\")\n",
        "print(f\"Padded Sequence:\\n{train_padded[0]}\\n\")"
      ],
      "metadata": {
        "id": "oLD0B8GrLtdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos la arquitectura del modelo.\n",
        "\n",
        "Observa la capa `Embedding` ([documentación](https://keras.io/api/layers/core_layers/embedding/)). Esta capa se encarga de asignar representaciones vectoriales (embeddings) a cada palabra, lo hace de manera implícita durante el entrenamiento. Otra alternativa es pasar directamente los embeddings pre-entrenados de palabras generados por *word2vec*, *FastText*, *GloVe*, etc."
      ],
      "metadata": {
        "id": "uuMGkHFD8HQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos la dimensión de los embeddings."
      ],
      "metadata": {
        "id": "CPwYQXOxXT5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100"
      ],
      "metadata": {
        "id": "_YTQg7LY4Dqd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construimos el modelo de red neuronal para esta tarea de clasificación. La red consiste de las siguientes partes:\n",
        "\n",
        "1. Capa de Embedding. Esta capa *traduce* las secuencias de índices a representaciones vectoriales densas de menor dimensión `embedding_dim`. Le especificamos el tamaño de las secuencias y el número de palabras del vocabulario.\n",
        "2. Célula de LSTM. Esta es la capa recurrente que irá recibiendo secuencialmente las palabras, una a una, de cada review y al final de la secuencia producirá una salida que irá a la siguiente capa.\n",
        "3. Red MLP. En esta parte de la red hay una red *fully connected* con capas densas.\n",
        "4. Capa de salida. Dado que es una clasificación binaria, al final tenemos una capa densa de 1 neurona prediciendo la probabilidad de que el review sea positivo. Dado que es una clasificación binaria, usamos la perdida `binary_crossentropy` y además, la métrica `accuracy`."
      ],
      "metadata": {
        "id": "3RshV--hIwdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Dropout\n",
        "from keras.layers import LSTM\n",
        "\n",
        "# ----- model initialization\n",
        "model = keras.Sequential([\n",
        "    Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    LSTM(100, dropout=0.2),\n",
        "    Dense(16, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# ----- compile model\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "X6Xmo4iEAeI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos un callback `EarlyStopping` para parar el entrenamiento de la red cuando la pérdida de validación empiece a aumentar. Observa el parámetro `patience`."
      ],
      "metadata": {
        "id": "fzuNsu1m7VKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)"
      ],
      "metadata": {
        "id": "pfGZfzrS2vMn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamos el modelo"
      ],
      "metadata": {
        "id": "Q_qiS1LtIaNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "\n",
        "history = model.fit(train_padded, train_labels,\n",
        "                    epochs=num_epochs, verbose=1,\n",
        "                    validation_split=0.1,\n",
        "                    callbacks=[es])"
      ],
      "metadata": {
        "id": "xU982WFBAgO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos las curvas de entrenamiento"
      ],
      "metadata": {
        "id": "ykCTxNHs8DkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 4),dpi=100)\n",
        "plt.suptitle(\"Training Curves\",fontsize=16)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.suptitle(\"Validation and Training Loss\",fontsize=14)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend()\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.suptitle(\"Validation and Training Accuracy\",fontsize=14)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='validation')\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "91zBpqvIytvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Obtenemos las predicciones y evaluamos el desempeño de la red"
      ],
      "metadata": {
        "id": "lnGjOjfxDu9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "¿Cómo se ven las predicciones?"
      ],
      "metadata": {
        "id": "5sEybYIh4ctU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_padded)\n",
        "\n",
        "print(predictions[:5])"
      ],
      "metadata": {
        "id": "_Iyt2VS6Ak3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como podemos ver, la predicción de la red LSTM para cada review es un valor $0 \\leq x \\leq 1$ (esto, ya que la activación es una sigmoide). Podemos interpretar este valor como la probabilidad que estima la red de que el review tenga la clase 1 (es decir, que el review tenga opinión \"positiva\").\n",
        "\n",
        "Entonces, para obtener las predicciones de las clases, asignamos la clase 1 si $x\\geq 0.5$ y clase 0 si $x<0.5$."
      ],
      "metadata": {
        "id": "52rmyyMU4vGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_labels = []\n",
        "\n",
        "for x in predictions:\n",
        "    if x >= 0.5:\n",
        "        pred_labels.append(1)\n",
        "    else:\n",
        "        pred_labels.append(0)\n",
        "\n",
        "print(pred_labels[:5])"
      ],
      "metadata": {
        "id": "cCsskJdX4ZES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluamos la calidad de las predicciones usando el accuracy y el recall."
      ],
      "metadata": {
        "id": "at5o4R897_LF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix\n",
        "\n",
        "test_accuracy = accuracy_score(test_labels,pred_labels)\n",
        "test_recall = recall_score(test_labels,pred_labels)\n",
        "print(f\"Test Accuracy: {round(test_accuracy,3)}\")\n",
        "print(f\"Test Recall: {round(test_recall,3)}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix:\\n\",confusion_matrix(test_labels,pred_labels))"
      ],
      "metadata": {
        "id": "bO1AqC1K5r7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, veamos algunas predicciones arbitrarias:"
      ],
      "metadata": {
        "id": "SAU457OM6QDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_size = len(test_reviews)\n",
        "\n",
        "idxs = np.random.choice(test_size,size=5,replace=False)\n",
        "\n",
        "for j in idxs:\n",
        "    print(\"Review:\")\n",
        "    print(df.loc[j,'review'])\n",
        "    print(f\"Label: {df.loc[j,'sentiment']}\")\n",
        "    print(f\"Predicted Label: {pred_labels[j]}\\n\")"
      ],
      "metadata": {
        "id": "rOi7dbki6OfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⭕ Ejercicio:\n",
        "\n",
        "* Modifica la arquitectura de la LSTM para mejorar el desempeño de la LSTM anterior.\n",
        "* Puedes usar también capas de dropout, callbacks, modificar las capas densas del final (recuerda que la capa final no se puede mover)."
      ],
      "metadata": {
        "id": "A80nr1Cvv38Z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nPeL6eZdCxkK"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}