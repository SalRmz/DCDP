{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP_2022/blob/main/03-Deep-Learning/notebooks/Tarea-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "_yytt8qLJYUK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Módulo 1 - Tarea 1\n",
        "\n",
        "Construir un clasificador MLP para predecir la clase de un documento de texto del corpus *20newsgroups* ([más información](http://qwone.com/~jason/20Newsgroups/)), el cual ya ha sido usado varias veces en clase.\n",
        "\n",
        "Para esto, puedes usar tantas capas como desees, además, puedes modificar el número de neuronas, hacer dropout, usar callbacks y gridsearch (puede ser muy tardado). También podrías aplicar reducción de dimensionalidad a alguna de las matrices de features obtenidas. La función de perdida para la clasificación multiclase que usaremos es `categorical_crossentropy` y la métrica de rendimiento será el accuracy.\n",
        "\n",
        "Una vez que tengas tu mejor modelo posible respecto al accuracy, reportar también las métricas:\n",
        "\n",
        "* Recall\n",
        "* Precision\n",
        "* Roc-Auc score\n",
        "* La matriz de confusión\n",
        "\n",
        "Divide el conjunto de datos en 85% de entrenamiento y 15% de prueba.\n",
        "\n",
        "**Fecha de entrega: Domingo 25 de junio**\n",
        "\n",
        "En el siguiente link puedes encontrar las mejores accuracy que se han logrado, así como la estrategia. https://paperswithcode.com/sota/text-classification-on-20news\n",
        "\n",
        "No es realista esperar obtener resultados de ese orden puesto que se trata de un problema no trivial. En sesiones posteriores iremos mejorando este resultado con algunas redes más especializadas."
      ],
      "metadata": {
        "id": "WigL21XQKS2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Obtener el dataset"
      ],
      "metadata": {
        "id": "CiiwmEnaEbuk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "\n",
        "full_data = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'),\n",
        "                               subset='all'\n",
        "                              )\n",
        "\n",
        "\n",
        "docs = full_data.data   # Los documentos\n",
        "y = full_data.target    # Las clases de los documentos"
      ],
      "metadata": {
        "id": "0aj8246FvH7m"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recuerda que para hacer clasificación multiclase con redes MLP necesitas tener las clases codificadas como *one-hot encoding*."
      ],
      "metadata": {
        "id": "wlGNGE9TIea_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Obtención de features\n",
        "\n",
        "Para obtener las features de cada documento usaremos dos estrategias:\n",
        "\n",
        "1. El módelo bolsa de palabras (*Bag of words: BOW*). Para esto usaremos la clase `CountVectorizer` de scikit-learn.\n",
        "2. El módelo TF-IDF. Para esto usaremos la clase `TfidfVectorizer` de scikit-learn. Los detalles de este modelo los puedes ver en la notebook de [clustering](https://github.com/DCDPUAEM/DCDP/blob/main/02-Machine-Learning/notebooks/12-Clustering.ipynb) del módulo pasado.\n",
        "\n",
        "Además, limpiaremos el texto y lematizaremos.\n",
        "\n",
        "> La lematización es un proceso lingüístico que consiste en, dada una forma flexionada (es decir, en plural, en femenino, conjugada, etc), hallar el lema correspondiente. El lema es la forma que por convenio se acepta como representante de todas las formas flexionadas de una misma palabra. Es decir, el lema de una palabra es la palabra que nos encontraríamos como entrada en un diccionario tradicional: singular para sustantivos, masculino singular para adjetivos, infinitivo para verbos. Por ejemplo, decir es el lema de dije, pero también de diré o dijéramos; guapo es el lema de guapas; mesa es el lema de mesas.\n",
        "\n"
      ],
      "metadata": {
        "id": "JalSsB9rEht8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bajamos las *stopwords*, signos de puntuación y el módulo *wordnet* (usado para sinónimos y lematización)."
      ],
      "metadata": {
        "id": "inrMdv76GIn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import download\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "2UJ6kLcS0n_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Limpiamos el texto quitando signos de puntuación, números, símbolos especiales y pasando todo a minúsculas. Usamos la función que ya habíamos definido en el módulo pasado."
      ],
      "metadata": {
        "id": "4-wtqdEwGmAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/DCDPUAEM/DCDP/main/02-Machine-Learning/data/limpiador_texto.py\"\n",
        "!wget --no-cache --backups=1 {url}"
      ],
      "metadata": {
        "id": "Xxo91KzssXig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from limpiador_texto import preprocesar_textos\n",
        "\n",
        "clean_docs = preprocesar_textos(docs)"
      ],
      "metadata": {
        "id": "R4L10paK0pNg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos como va el corpus hasta el momento"
      ],
      "metadata": {
        "id": "cQk-NHaKGwRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "docs_df = pd.DataFrame(data={'document': clean_docs,\n",
        "                             'class': y})\n",
        "docs_df"
      ],
      "metadata": {
        "id": "hDpUrapR9Zle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lematizamos"
      ],
      "metadata": {
        "id": "oWASkpEFGjGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    return(\" \".join([lemmatizer.lemmatize(w,\"v\") for w in w_tokenizer.tokenize(text)]))\n",
        "\n",
        "docs_df[\"lemmatized_document\"] = docs_df['document'].apply(lemmatize_text)\n",
        "docs_df"
      ],
      "metadata": {
        "id": "dvx-IiJy9Yue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos una lista con los documentos preprocesados"
      ],
      "metadata": {
        "id": "0NN1gqKyGzxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_docs = list(docs_df[\"lemmatized_document\"].values)"
      ],
      "metadata": {
        "id": "6EmIK4ql-j4L"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. El modelo"
      ],
      "metadata": {
        "id": "redZtrF0GADJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1  TF-IDF"
      ],
      "metadata": {
        "id": "LeCmGC3L8Az0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtenemos la matriz de features TF-IDF. Recuerda que `topn_words` especifica cuántas palabras del vocabulario tomar, ordenadas por las más frecuentes."
      ],
      "metadata": {
        "id": "adGFiSMFHiTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "topn_words = 800\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english',\n",
        "                             max_features=topn_words)\n",
        "X_tfidf = vectorizer.fit_transform(clean_docs)\n",
        "X_tfidf = np.asarray(X_tfidf.todense())\n",
        "print(X_tfidf.shape)"
      ],
      "metadata": {
        "id": "D9e8xwjD0sNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tu modelo ..."
      ],
      "metadata": {
        "id": "t6GdLN9nHvrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 BOW"
      ],
      "metadata": {
        "id": "RnfPD_Cx3nD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "topn_words = 800\n",
        "\n",
        "count_vectorizer = CountVectorizer(stop_words='english',\n",
        "                                    max_features=topn_words)\n",
        "X_counts = count_vectorizer.fit_transform(clean_docs)\n",
        "X_counts =  np.asarray(X_counts.todense())\n",
        "print(X_counts.shape)"
      ],
      "metadata": {
        "id": "hFQug9Q-0_3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tu modelo ..."
      ],
      "metadata": {
        "id": "NOZeHt33H0_2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}