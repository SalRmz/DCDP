{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TgLUQd5WLOm"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/main/03-Deep-Learning/notebooks/05-CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFc78aDaWLOo"
      },
      "source": [
        "<h1>Clasificación con Redes Neuronales Convolucionales</h1>\n",
        "\n",
        "En esta notebook usaremos una red neuronal convolucional (CNN) para clasificar el dataset *cats vs dogs* de kaggle. Observaremos, además, el efecto del dropout y analizaremos la información de las capas ocultas para ganar intuición sobre el funcionamiento interno de este tipo de redes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq-bgFf-WLOq"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XN71Ul1JWLOx"
      },
      "source": [
        "Verifiquemos que el entorno de ejecución en Colab sea GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ln6imMSY8vLe",
        "outputId": "e13e2ecc-08bc-4208-c261-46364a4792b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU presente en: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print('GPU presente en: {}'.format(tf.test.gpu_device_name()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EjrtHWCWLOv"
      },
      "source": [
        "# [El dataset Dogs vs. Cats](https://www.kaggle.com/c/dogs-vs-cats/overview)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG3DcGuhdyaT"
      },
      "source": [
        "El conjunto de datos de Dogs vs Cats fue publicado por Kaggle como parte de una competencia de visión computacional a fines de 2013, cuando las CNNs no eran muy comunes.\n",
        "\n",
        "Se puede descargar el dataset original en: https://www.kaggle.com/c/dogs-vs-cats/data.\n",
        "\n",
        "Esta notebook se puede usar con dos conjuntos de datos:\n",
        "\n",
        "* Usaremos el conjunto de datos original de entrenamiento, dado que contiene las etiquetas de las clases. Este conjunto contiene 25,000 imágenes de perros y gatos (12,500 de cada clase) y tiene un tamaño de 543 MB. Ya se encuentra dividido en *train*, *validation* y *test*. [Download](https://drive.google.com/file/d/1Q3xOfn2Up9uIOLviS66oYH_oFFK-IGpW/view?usp=sharing)\n",
        "\n",
        "* Usaremos un conjunto reducido de datos, el cual contiene 1000 imágenes de cada clase para entrenamiento, 500 para validación y 500 para prueba. Todos los datos se sacaron del conjunto de entrenamiento original. [Download](https://drive.google.com/file/d/1Ce3u8dwYYriLkz5OpcGn72xIQENIHZX5/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C4obFfa2rjL"
      },
      "source": [
        "Copiaremos el dataset desde un vínculo de Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsjdNtihfL8n"
      },
      "outputs": [],
      "source": [
        "!pip install -qq gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC-8WGEgidmb"
      },
      "source": [
        "Descargamos el dataset desde Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LDaaZ3udhLI"
      },
      "outputs": [],
      "source": [
        "# ----- Versión completa -----\n",
        "# !gdown --id 1Q3xOfn2Up9uIOLviS66oYH_oFFK-IGpW\n",
        "\n",
        "# ----- Copia de la versión completa -----\n",
        "# !gdown 1hchhNQ_3WNncaXVD3kX58EIppcYFt-E2\n",
        "\n",
        "# ----- Versión reducida -----\n",
        "!gdown 1Ce3u8dwYYriLkz5OpcGn72xIQENIHZX5\n",
        "\n",
        "# ----- Copia de la versión reducida -----\n",
        "# !gdown 1NK9LvrVwsEQM0UHkFHq_GYCF2fjGrwAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXbCkAPa2w66"
      },
      "source": [
        "Descomprimimos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6G3WswyR8vho"
      },
      "outputs": [],
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "# file_name = '/content/cnn_perros_gatos.zip'\n",
        "# file_name = '/content/cnn_perros_gatos-copia.zip'\n",
        "file_name = '/content/cnn_perros_gatos-small.zip'\n",
        "# file_name = '/content/cnn_perros_gatos-small-copia.zip'\n",
        "\n",
        "with ZipFile(file_name, 'r') as myzip:\n",
        "    myzip.extractall()\n",
        "    print('Listo')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos algunas imágenes del dataset, como podemos ver:\n",
        "\n",
        "* Son archivos jpeg\n",
        "* Tienen diferentes tamaños"
      ],
      "metadata": {
        "id": "TtPRJtxIhLJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ipyplot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EVbHiCae8nD",
        "outputId": "3b97c598-d31b-403a-e0eb-3226f2ca4e9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.3/1.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import ipyplot\n",
        "import random, os\n",
        "\n",
        "path_1 = '/content/cnn_perros_gatos/train/cats'\n",
        "path_2 = '/content/cnn_perros_gatos/train/dogs'\n",
        "\n",
        "filenames_1 = random.sample(os.listdir(path_1), 5)\n",
        "filenames_2 = random.sample(os.listdir(path_2), 5)\n",
        "\n",
        "full_filenames_1 = [os.path.join(path_1, fname) for fname in filenames_1]\n",
        "full_filenames_2 = [os.path.join(path_2, fname) for fname in filenames_2]\n",
        "\n",
        "filenames = full_filenames_1 + full_filenames_2\n",
        "images_list = [Image.open(fname) for fname in filenames]\n",
        "\n",
        "ipyplot.plot_images(images_list,show_url=False,)"
      ],
      "metadata": {
        "id": "6WY6JsL3ecRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFqw4JUe-P2O"
      },
      "source": [
        "Exploramos las carpetas de entrenamiento, validación y prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMypQXip8vzN"
      },
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "\n",
        "train_dogs = 'cnn_perros_gatos/train/dogs'\n",
        "print('Para entrenamiento:')\n",
        "print('{} Perros.'.format(len(os.listdir(train_dogs))))\n",
        "train_cats = 'cnn_perros_gatos/train/cats'\n",
        "print('{} Gatos.'.format(len(os.listdir(train_cats))))\n",
        "print('\\nPara validación:')\n",
        "validation_dogs = 'cnn_perros_gatos/validation/dogs'\n",
        "print('{} Perros.'.format(len(os.listdir(validation_dogs))))\n",
        "validation_cats = 'cnn_perros_gatos/validation/cats'\n",
        "print('{} Gatos.'.format(len(os.listdir(validation_cats))))\n",
        "print('\\nPara prueba:')\n",
        "test_dogs = 'cnn_perros_gatos/test/dogs'\n",
        "print('{} Perros.'.format(len(os.listdir(test_dogs))))\n",
        "test_cats = 'cnn_perros_gatos/test/cats'\n",
        "print('{} Gatos.'.format(len(os.listdir(test_cats))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4aC0_mXDbXn"
      },
      "source": [
        "Definimos los directorios de entrenamiento, validación y prueba para usaralos en el resto de la notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJGFJS-qNLJc"
      },
      "outputs": [],
      "source": [
        "train_dir = 'cnn_perros_gatos/train'\n",
        "validation_dir = 'cnn_perros_gatos/validation'\n",
        "test_dir = 'cnn_perros_gatos/test'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phDgmrWsWLO6"
      },
      "source": [
        "# Modelo 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZFIorONDhG9"
      },
      "source": [
        "Definimos un primer modelo de CNN. Usaremos las capas `Conv2D` para las operaciones de convolución y `MaxPooling2D` para el pooling.\n",
        "\n",
        "* https://keras.io/api/layers/convolution_layers/convolution2d/\n",
        "* https://keras.io/api/layers/pooling_layers/max_pooling2d/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yySANB-NNr4w"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, 3, activation='relu',\n",
        "                           input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSL8Y3n7rLtL"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbnp2bbL9jes"
      },
      "source": [
        "* NOTA que comenzamos con imágenes de tamaño 150 x 150 (una elección de tamaño arbitraria) y terminamos con mapas de características de tamaño 7 x 7 justo antes de la capa de *flatten*.\n",
        "* En realidad las imágenes de entrada tienen tamaños diversos (desconocidos), pero afortunadamente Keras nos puede ayudar a pre-procesarlas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA43G8SqEv2f"
      },
      "source": [
        "Compilamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94vCX8FgrPgU"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_Zxc_HG1MuG"
      },
      "source": [
        "## Preprocesamiento de datos\n",
        "\n",
        "\n",
        "Los datos deben formatearse en tensores de punto flotante preprocesados adecuadamente antes de que se introduzcan en la red. En este momento, nuestros datos se encuentran almacenados como archivos JPEG, por lo que los pasos para que puedan ser introducidos en nuestra red son:\n",
        "\n",
        "* Leer los archivos de imagen.\n",
        "\n",
        "* Decodificar el contenido JPEG a cuadrículas de píxeles RBG.\n",
        "\n",
        "* Convertirlos en tensores de punto flotante.\n",
        "\n",
        "* Volver a escalar los valores de píxeles (entre 0 y 255) al intervalo $[0, 1]$ (las redes neuronales prefieren tratar con valores de entrada pequeños).\n",
        "\n",
        "Afortunadamente, Keras tiene herramientas para encargarse de estos pasos automáticamente. Keras tiene un módulo con herramientas de ayuda para procesamiento de imágenes, ubicado en **keras.preprocessing.image**. En particular, contiene la clase **ImageDataGenerator**, que permite configurar rápidamente los generadores de Python que pueden convertir automáticamente los archivos de imagen en disco en *batches* de tensores preprocesados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LAZtTFHvkvZ",
        "outputId": "8a2f8ced-bc22-4707-8cd8-36e0511396f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Todas las imágenes serán reescaladas por 1./255.\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,              # Directorio donde buscará las imagenes\n",
        "        target_size=(150, 150), # Todas las imágenes se redimensionarán a 150 x 150\n",
        "        batch_size=20,\n",
        "        class_mode='binary'     # Es una tarea de clasificación binaria\n",
        "        )\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHKu7G_y4fPC"
      },
      "source": [
        "* Vamos a revisar la salida de uno de estos generadores: produce batches de imágenes de 150 x 150 RGB (con la forma (20, 150, 150, 3)) y etiquetas binarias (con la forma (20,)). 20 es el número de muestras en cada batch (el tamaño del batch).\n",
        "* Como el generador genera estos batches de forma indefinida (i.e. recorre sin fin las imágenes presentes en la carpeta que se le indicó), se necesita romper el loop de iteración en algún punto. A continuación podemos ver cómo es cada corrida (batch/step) que proporciona el generador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51xbh9LMv0yQ",
        "outputId": "4c41a255-e85c-4841-b9b8-b31f2697280f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimensiones del batch de imágenes: (20, 150, 150, 3)\n",
            "Dimensiones del batch de las etiquetas: (20,)\n"
          ]
        }
      ],
      "source": [
        "for data_batch, labels_batch in train_generator:\n",
        "    print('Dimensiones del batch de imágenes:', data_batch.shape)\n",
        "    print('Dimensiones del batch de las etiquetas:', labels_batch.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LevQH8ih4ek_"
      },
      "source": [
        "* Vamos a proceder a entrenar nuestro modelo con los datos usando el generador. Debido a que los datos se generan infinitamente, el generador necesita saber cuántas muestras extraer antes de declarar una época finalizada. Esta es la función del argumento **steps_per_epoch**\n",
        "\n",
        "* En este caso, **steps_per_epoch** corresponde al número de batches que requiere el generador para leer el conjunto de datos completo. Sólo después de haber solicitado este número de batches, el proceso de ajuste de nuestro modelo pasará a la siguiente época. **steps_per_epoch** corresponde a el número de pasos de descenso del gradiente. En nuestro caso, cada batch tiene un tamaño de 20 muestras, por lo que tomará 100 pasos (batches) hasta que cubramos las 2,000 muestras de nuestra base de datos.\n",
        "\n",
        "* Como siempre, uno puede pasar un argumento llamado **validation_data**. Es importante destacar que este argumento puede ser un generador de datos en sí mismo, pero también podría ser una tupla de arreglos Numpy. Si se pasa un generador como **validation_data**, entonces se espera que este generador produzca batches de datos de validación sin fin, y por lo tanto también se debe especificar el argumento **validation_steps**, que le dice al proceso cuántos batches debe extraer del generador de validación para su evaluación."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento del modelo"
      ],
      "metadata": {
        "id": "505HhEEzMn8s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdjhzcgzwQ-T"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,\n",
        "      epochs=30,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOeKcGvvWLPB"
      },
      "source": [
        "## Rendimiento del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtq39jKpFV2g"
      },
      "source": [
        "Guardamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFnMW2EbyyqD"
      },
      "outputs": [],
      "source": [
        "model.save('cnn_perros_gatos_1.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OFYSGmOGZsF"
      },
      "source": [
        "Grafiquemos las curvas de aprendizaje"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lt6kRFIYzjPb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---- graficamos la función de perdida ----\n",
        "plt.figure(figsize=(11,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Validation and Training Loss\",fontsize=14)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.legend()\n",
        "# ---- graficamos la métrica de rendimiento ----\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"Validation and Training Accuracy\",fontsize=14)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(validation_generator)"
      ],
      "metadata": {
        "id": "VmNuNQb-IC6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo 2"
      ],
      "metadata": {
        "id": "y4GxjIoaMv_t"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXzBZmaj_V2_"
      },
      "source": [
        "## Aumento de Datos\n",
        "\n",
        "* El efecto de sobreajuste ocurre cuando se tienen muy pocas muestras de las que aprender, lo que nos impide entrenar un modelo capaz de generalizar a nuevos datos. Si tuviesemos datos infinitos, nuestro modelo estaría expuesto a todos los aspectos posibles de la distribución de datos en cuestión y nunca se sobreajustaría nuestro modelo.\n",
        "\n",
        "* El aumento de datos adopta el enfoque de generar más datos de entrenamiento a partir de muestras de entrenamiento existentes, al \"aumentar\" las muestras a través de una serie de transformaciones aleatorias que producen imágenes de apariencia creíble. El objetivo es que durante el tiempo de entrenamiento, nuestro modelo nunca vea exactamente la misma imagen dos veces. Esto ayuda a que el modelo se exponga a más aspectos de los datos y generalice mejor.\n",
        "\n",
        "* En Keras, esto se puede hacer configurando una serie de transformaciones aleatorias que se realizarán en las imágenes leídas por nuestra instancia de ImageDataGenerator.\n",
        "\n",
        "* Vamos a comenzar por aumentar una imagen.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3_qyda81Q81"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U48tZWbBB2TM"
      },
      "source": [
        "Las opciones anteriores son solo algunas de las opciones disponibles.\n",
        "\n",
        "* **rotation_range** es un valor en grados (0-180), un rango dentro del cual girar las imágenes de forma aleatoria.\n",
        "\n",
        "* **width_shift** y **height_shift** son rangos expresados como una fracción del ancho o altura total de la imagen, dentro de los cuales se pueden trasladar vertical u horizontalmente de forma aleatoria a las imágenes.\n",
        "\n",
        "* **shear_range** aplica aleatoriamente transformaciones de corte.\n",
        "\n",
        "* **zoom_range** aplica acercamientos aleatorios dentro de las imágenes.\n",
        "\n",
        "* **horizontal_flip** Voltea de forma aleatoria la mitad en las imágenes horizontalmente.\n",
        "\n",
        "* **fill_mode** es la estrategia utilizada para rellenar píxeles creados, que pueden aparecer después de una rotación o un cambio de ancho / altura."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbyqp-q61cbr"
      },
      "outputs": [],
      "source": [
        "from keras import utils\n",
        "import random\n",
        "\n",
        "train_cats_dir = '/content/cnn_perros_gatos/train/cats' # El directorio donde están las imágenes de gatos de entrenamiento\n",
        "fnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)]\n",
        "img_path = random.sample(fnames, 1)[0] # Escogemos una imagen al azar para aplicar el \"aumentado de datos\"\n",
        "img = utils.load_img(img_path, target_size=(150, 150)) # Leemos la imagen y la redimensionamos.\n",
        "x = utils.img_to_array(img) # Leemos la imagen y la redimensionamos.\n",
        "x = x.reshape((1,) + x.shape) # Redimensionamos el arreglo a (1, 150, 150, 3)\n",
        "\n",
        "'''\n",
        "El comando .flow () genera batches de imágenes transformadas aleatoriamente\n",
        "Con el \"for\" de abajo estaremos en un loop indefinidamente,\n",
        "Necesitamos 'romper' el loop en algún momento\n",
        "'''\n",
        "\n",
        "fig, axs = plt.subplots(5,5,figsize=(10,10))\n",
        "k = 0\n",
        "for batch in datagen.flow(x, batch_size=1):\n",
        "    i = k//5\n",
        "    j = k%5\n",
        "    axs[i,j].imshow(utils.array_to_img(batch[0]))\n",
        "    axs[i,j].axis('off')\n",
        "    k += 1\n",
        "    if k == 25:\n",
        "        break\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teFUV-JSFKZH"
      },
      "source": [
        "* Si entrenamos una nueva red neuronal utilizando esta configuración de aumento de datos, nuestra red nunca verá dos veces la misma entrada, pues a cada nueva imagen se le aplica transformaciones aleatorias dentro de ciertos rangos.\n",
        "\n",
        "* Sin embargo, las entradas que ves están aún muy interrelacionadas, ya que provienen de un pequeño número de imágenes originales: **no podemos producir nueva información, sólo podemos mezclar la información existente**.\n",
        "\n",
        "* Dado que esto podría no ser suficiente para librarnos del sobreajuste. Para mitigarlo aún más, también agregaremos una capa de Dropout a nuestro modelo, justo antes de la etapa del clasificador densamente conectado (fully-connected)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ5RY_qYGlln"
      },
      "source": [
        "Definimos una nueva red más profundo y con dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYgaCOd2R4HU"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbgrTc9UV5i-"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6bwra-5TfZ3"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento"
      ],
      "metadata": {
        "id": "-BL7MuurM3p2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHiNZWb2GqmS"
      },
      "source": [
        "Definimos los nuevos generadores de imágenes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hc8gia_TwWn"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,)\n",
        "\n",
        "# Debemos notar que los datos del conjunto validación no son sometidos\n",
        "# al aumentado de datos\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        # Esta es la carpeta de origen\n",
        "        train_dir,\n",
        "        # Todas las imágenes se redimensionarán a 150 x 150\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "# Observación, para mejores resultados entrenar con epochs = 100\n",
        "# Por cuestiones de tiempo se entrena con epochs = 30\n",
        "\n",
        "history = model.fit(\n",
        "                train_generator,\n",
        "                steps_per_epoch=100,\n",
        "                epochs=30,\n",
        "                validation_data=validation_generator,\n",
        "                validation_steps=50,\n",
        "                verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5VV-0J0UTGN"
      },
      "outputs": [],
      "source": [
        "# Descomentar la siguiente linea si entrenaste con epochs = 100\n",
        "model.save('cnn_perros_gatos_extra_imgs.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CHs_DRqGwKk"
      },
      "source": [
        "Veamos las curvas de entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pYHbXOlUXPx"
      },
      "outputs": [],
      "source": [
        "# ---- graficamos la función de perdida ----\n",
        "plt.figure(figsize=(11,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.title(\"Validation and Training Loss\",fontsize=14)\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.legend()\n",
        "# ---- graficamos la métrica de rendimiento ----\n",
        "plt.subplot(1,2,2)\n",
        "plt.title(\"Validation and Training Accuracy\",fontsize=14)\n",
        "plt.plot(history.history['accuracy'], label='train')\n",
        "plt.plot(history.history['val_accuracy'], label='validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(validation_generator)"
      ],
      "metadata": {
        "id": "L4A9vsXhIAh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_-OWUThLg-f"
      },
      "source": [
        "## ¿Qué pasaría si entrenas con 100 épocas?\n",
        "\n",
        "Con 100 épocas se obtendrían los siguientes resultados:\n",
        "\n",
        "![](https://drive.google.com/uc?id=1tqFh1ETyhtOE3ttAIkjk8cqFy-e-SLR_)\n",
        "\n",
        "El accuracy fue de 80.1%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos descargar este modelo ya entrenado de Google Drive"
      ],
      "metadata": {
        "id": "RVHUfTj5Ibcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1m5kfBviSBa6dI9wEwBpowwlBJ1EUdds0"
      ],
      "metadata": {
        "id": "gjHZ07i3IQsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKDZA5MnVnt3"
      },
      "source": [
        "## ➖ Visualizando los mapas de características de una red neuronal convolucional\n",
        "\n",
        "Algunos señalan que los modelos de aprendizaje profundo funcionan como \"cajas negras\", pues aprenden representaciones que son difíciles de extraer y presentar de una forma legible para el ser humano.\n",
        "\n",
        "Si bien esto es parcialmente cierto para algunos tipos de modelos de aprendizaje profundo, definitivamente no lo es para las redes convolucionales (*CNN*). Las representaciones aprendidas por las redes convolucionales son altamente susceptibles de visualización, en gran parte porque son representaciones de conceptos visuales. Desde 2013, se ha desarrollado una amplia gama de técnicas para visualizar e interpretar estas representaciones. No exploraremos todas ellas, pero mencionaremos tres de las más accesibles y útiles:\n",
        "\n",
        "\n",
        "\n",
        "*   **Visualización de las salidas intermedias de una *CNN*  (\"activaciones intermedias\")**. Este método es útil para entender cómo las capas sucesivas de una red convolucional transforman su entrada y para obtener una noción de la función de los filtros individuales en una red convolucional .\n",
        "\n",
        "*   **Visualización de los  filtros en una CNN**. Este método es útil para entender con precisión a qué patrón o concepto visual es receptivo cada filtro en una red convolucional.\n",
        "\n",
        "*   **Visualización de los mapas de calor de activación por clase en una imagen**. Este método es útil para entender qué parte de una imagen se identificó como perteneciente a una clase determinada y, por lo tanto, permite localizar objetos en imágenes.\n",
        "\n",
        "En este ejercicio, abordaremos únicamente el primer método, la visualización de las activaciones intermedias o mapas de características. Para ello, usaremos la CNN que entrenamos anteriormente.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos seguirlo usando o leerlo desde un archivo"
      ],
      "metadata": {
        "id": "COssYGVAI3mt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfXwuYwPda0Q",
        "outputId": "17219a04-3c92-40f2-d52f-6fd0023f43e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 148, 148, 32)      896       \n",
            "                                                                 \n",
            " max_pooling2d_4 (MaxPooling  (None, 74, 74, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 72, 72, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_5 (MaxPooling  (None, 36, 36, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 34, 34, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPooling  (None, 17, 17, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 15, 15, 128)       147584    \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPooling  (None, 7, 7, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 6272)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 6272)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               3211776   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,453,121\n",
            "Trainable params: 3,453,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# model = tf.keras.models.load_model('Data/cnn_perros_gatos_2.h5')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qczIqCMEzRcb"
      },
      "source": [
        "Seleccionaremos una imagen de entrada, puede ser cualquier imagen del **conjunto de test**. Por ser del conjunto de test, no forma parte de las imágenes sobre las que se entrenó la red.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2CcwGp7dcMD",
        "outputId": "c3abbb1f-daa8-4c9f-9bc9-bb1de33991a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 150, 150, 3)\n"
          ]
        }
      ],
      "source": [
        "from keras import utils\n",
        "import numpy as np\n",
        "\n",
        "# ----- Para el conjunto de datos completo ----\n",
        "# img_path = 'cnn_perros_gatos/test/cats/cat.147.jpg'   # Una imágen de un gato\n",
        "# img_path = 'cnn_perros_gatos/test/dogs/dog.1517.jpg'  # Una imágen de un perro\n",
        "\n",
        "# ----- Para el conjunto de datos reducido ----\n",
        "# img_path = '/content/cnn_perros_gatos/test/cats/cat.10128.jpg'\n",
        "img_path = '/content/cnn_perros_gatos/test/dogs/dog.10086.jpg'\n",
        "\n",
        "# ----- Preprocesamos la imagen en un tensor 4D\n",
        "\n",
        "img = utils.load_img(img_path, target_size=(150, 150))\n",
        "img_tensor = utils.img_to_array(img)\n",
        "img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "\n",
        "# ----- Debemos recordar que el modelo fue entrenado con imagenes de entrada preprocesadas de la siguiente manera:\n",
        "img_tensor /= 255.\n",
        "\n",
        "# Debemos ver que su forma es de (1, 150, 150, 3)\n",
        "print(img_tensor.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjD5E3W05kjX"
      },
      "source": [
        "Mostramos la imágen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRaoI0vSdfcF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.imshow(img_tensor[0])\n",
        "plt.axis('Off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZH5ebeoL56FH"
      },
      "source": [
        "* Para extraer los mapas de características que queremos visualizar, crearemos un modelo de Keras que toma lotes ó *batches* de imágenes como entrada y genera las activaciones de todas las capas de convolución y *pooling*.\n",
        "* Para ello, utilizaremos la clase de Keras **Model**, que ya vimos anteriormente. Un **model** se instancia mediante dos argumentos: un tensor de entrada (o lista de tensores de entrada) y un tensor de salida (o lista de tensores de salida). La clase resultante es un modelo de Keras, igual que los modelos secuenciales (Sequential models) que ya estudiamos, que mapea las entradas especificadas a las salidas especificadas. Lo que distingue a la clase **Model** es que permite modelos con múltiples salidas, a diferencia de **Sequential**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiDTipp_diyQ"
      },
      "outputs": [],
      "source": [
        "# Extraemos las salidas de las 8 capas superiores:\n",
        "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
        "# Creamos un modelo que devolverá estas salidas, dada la entrada al modelo:\n",
        "activation_model = tf.keras.models.Model(inputs=model.input, outputs=layer_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB4r9KkU9IEy"
      },
      "source": [
        "* Cuando se introduce una imagen como entrada a la red, este modelo devuelve los valores de las activaciones de las capas del modelo original. Hasta antes de esta sección del ejercicio, el modelo que se presentó sólo tenía exactamente una entrada y una salida. Ahora estamos introduciendo el concepto de un modelo con múltiples salidas.\n",
        "\n",
        "* En el caso general, un modelo podría tener cualquier número de entradas y salidas. Este último modelo tiene una entrada y 8 salidas, una salida por capa de activación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwuC7FEWdl8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e6e04ca-1bc7-4201-cb43-3ea2d2e29e19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 273ms/step\n"
          ]
        }
      ],
      "source": [
        "# Esto devolverá una lista de arreglos de Numpy: Un arreglo por capa de activación\n",
        "activations = activation_model.predict(img_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4--_UfiCP1O"
      },
      "source": [
        "Por ejemplo, vamos a imprimir las dimensiones  del mapa de características/activaciones de la primer capa convolucional para la imagen del gato:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnYfbrz1doar"
      },
      "outputs": [],
      "source": [
        "first_layer_activation = activations[0]\n",
        "print(first_layer_activation.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z460Fyq3TWJm"
      },
      "source": [
        "Es un mapa de características con una dimensión de 148 x 148 con 32 canales o profundidad.\n",
        "\n",
        "Vamos a visualizar el 3er canal de ese mapa de características."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAMBCCJxdqj1"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.matshow(first_layer_activation[0, :, :, 2], cmap='plasma')\n",
        "plt.axis('Off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3ICdttbWbPW"
      },
      "source": [
        "Probemos algún otro canal, pero debemos tener en cuenta que los canales que tenga uno pueden variar de los que tiene cualquier otro, ya que los filtros que en específico aprende la red en las capas convolucionales no son determinísticos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1pjytgcdu_B"
      },
      "outputs": [],
      "source": [
        "plt.matshow(first_layer_activation[0, :, :, 30], cmap='viridis')\n",
        "plt.axis('Off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhiUH3GhX29w"
      },
      "source": [
        "Finalmente, vamos a desplegar un gráfico completo de todas las activaciones en la red. En otras palabras, vamos a extraer y mostrar cada canal presente en cada uno de nuestros 8 mapas de características. Apilaremos los resultados en un gran tensor de imagen, con los canales colocados uno junto al otro.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA8QlWTZNVyI"
      },
      "source": [
        "Primero, guardamos los nombres de las capas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU3C1qdgdyQs"
      },
      "outputs": [],
      "source": [
        "layer_names = []\n",
        "for layer in model.layers[:8]:\n",
        "    layer_names.append(layer.name)\n",
        "\n",
        "images_per_row = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tI1_CVYgd2QW"
      },
      "outputs": [],
      "source": [
        "for layer_name, layer_activation in zip(layer_names, activations):\n",
        "    # Este es el número de características presentes en un mapa de características\n",
        "    n_features = layer_activation.shape[-1]\n",
        "\n",
        "    # El mapa de características tiene la forma: (1, size, size, n_features)\n",
        "    size = layer_activation.shape[1]\n",
        "\n",
        "    # Vamos a colocar los canales de activación en esta matriz\n",
        "    n_cols = n_features // images_per_row\n",
        "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
        "\n",
        "    # Colocaremos cada mapa en esta gran malla horizontal\n",
        "    for col in range(n_cols):\n",
        "        for row in range(images_per_row):\n",
        "            channel_image = layer_activation[0, :, :, col * images_per_row + row]\n",
        "\n",
        "            # Procesaremos el mapa de características para que sea visualmente agradable\n",
        "            channel_image -= channel_image.mean()\n",
        "            channel_image /= channel_image.std()\n",
        "            channel_image *= 64\n",
        "            channel_image += 128\n",
        "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
        "            display_grid[col * size : (col + 1) * size,\n",
        "                         row * size : (row + 1) * size] = channel_image\n",
        "\n",
        "    # Mostramos los mapas en la malla\n",
        "    scale = 1. / size\n",
        "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
        "                        scale * display_grid.shape[0]))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.axis('Off')\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh6Ip2qTYete"
      },
      "source": [
        "# Observaciones\n",
        "\n",
        "Del gráfico de mapas de características podemos notar lo siguiente:\n",
        "\n",
        "* La primera capa de la red actúa como una colección de varios detectores de borde. En esa etapa, las activaciones aún retienen casi toda la información presente en la imagen inicial.\n",
        "\n",
        "* A medida que avanzamos en profundidad, las activaciones se vuelven cada vez más abstractas y menos interpretables visualmente. Se comienzan a codificar conceptos de nivel superior como \"oreja de gato\" u \"ojo de gato\". Las representaciones superiores llevan cada vez menos información sobre el contenido visual de la imagen, y cada vez más información relacionada con la clase de la imagen.\n",
        "\n",
        "* La escasez de activaciones aumenta con la profundidad de la red: en la primera capa, todos los filtros se activan mediante la imagen de entrada, pero en las siguientes capas, más y más canales de activación están en blanco. Esto significa que el patrón codificado por el filtro no se encuentra en la imagen de entrada.\n",
        "\n",
        "# Comentarios Finales\n",
        "\n",
        "Acabamos de evidenciar un hecho muy importante de las representaciones aprendidas por las redes neuronales profundas: las características extraídas por una capa se vuelven cada vez más abstractas con la profundidad de la red.\n",
        "\n",
        "Las activaciones de las capas superiores contienen cada vez menos información sobre la entrada específica que se está viendo y más información sobre el objetivo (en el caso de este ejemplo, la clase de la imagen: gato o perro). Una red neuronal profunda actúa efectivamente como un *pipeline* (tubería) que destila la información, con datos en crudo que entran (en nuestro caso, imágenes RBG) y se transforman repetidamente de tal forma que la información irrelevante es filtrada (por ejemplo, la apariencia visual específica de la imagen) mientras que la información útil es magnificada y refinada (por ejemplo, la clase de la imagen).\n",
        "\n",
        "Esto es análogo a la forma en que los humanos y los animales perciben el mundo: después de observar una escena durante unos segundos, un humano puede recordar qué objetos abstractos estaban presentes en él (por ejemplo, una bicicleta, un árbol) pero muchas veces no puede recordar la apariencia específica de estos objetos.\n",
        "\n",
        "El cerebro ha aprendido a abstraer completamente la información visual, a transformarla en conceptos visuales de alto nivel mientras filtra por completo los detalles visuales irrelevantes, haciendo que sea tremendamente difícil recordar cómo se ven exactamente las cosas a nuestro alrededor.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5R3eylJLMUNs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}