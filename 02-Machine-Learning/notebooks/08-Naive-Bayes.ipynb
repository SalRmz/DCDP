{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m2L9c0IfnGA"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/master/02-Machine-Learning/notebooks/08-Naive-Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRzVC46XfnGD"
      },
      "source": [
        "# Clasificador Naive Bayes\n",
        "\n",
        "En esta notebook implementaremos un clasificador Naive-Bayes para resolver problemas de clasificación de texto. Además, mostraremos cómo usar la implementación de scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNj7498efnGF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evFtGtWUfnGG"
      },
      "source": [
        "Traemos la carpeta 'data' desde el repositorio de github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kl1KHqkNfnGH"
      },
      "outputs": [],
      "source": [
        "# !apt-get -qq install subversion\n",
        "!apt-get -qq install > /dev/null subversion\n",
        "\n",
        "!svn checkout \"https://github.com/DCDPUAEM/DCDP/trunk/02-Machine-Learning/data/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KYy8TtafnGI"
      },
      "source": [
        "## Archivo de datos preprocesados\n",
        "\n",
        "* El dataframe contiene una matriz BOW.\n",
        "\n",
        "$$\\begin{array}{cccc}\n",
        "    & doc_1 & ... & doc_n \\\\\n",
        "w_1 & 0 & ... & 1 \\\\\n",
        "... & ... & ... & ... \\\\\n",
        "w_k & 2 & ... & 1 \n",
        "\\end{array}$$\n",
        "\n",
        "donde $w_i$ es la $i$-sima palabra en el corpus y $doc_j$ es el $j$-simo documento (email).\n",
        "\n",
        "* Son 1000 correos y 9406 palabras.\n",
        "\n",
        "* Los correos vienen etiquetados de la siguiente forma: `correo_enron1_<0/1>_<#email>` donde \n",
        "\n",
        "    * 0: Ham \n",
        "    * 1: Spam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2J13sCMQfnGI"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('data/frecuencia_palabras.csv',index_col=0) # lectura del documento con pandas\n",
        "\n",
        "print(data.shape)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algunas palabras"
      ],
      "metadata": {
        "id": "B2Hphm_BWHNM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(data['WORD'].values[:15])"
      ],
      "metadata": {
        "id": "yby-FYIsV96N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d56CAe3fnGJ"
      },
      "source": [
        "Extraemos $X$, $y$ del conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.array([0 if 'correo_enron1_0' in col else 1 for col in data.columns.to_list()[1:]])\n",
        "\n",
        "X = data.iloc[:,1:].values.T\n",
        "\n",
        "print(X.shape,y.shape)"
      ],
      "metadata": {
        "id": "ZIh8B9grj-dG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separamos en conjunto de entrenamiento y prueba"
      ],
      "metadata": {
        "id": "RvBecjqZtx63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
        "                                                    test_size=0.2,random_state=40)"
      ],
      "metadata": {
        "id": "ozQpp77mtxON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMIu9bJ-fnGN"
      },
      "outputs": [],
      "source": [
        "print(f\"Correos de Entrenamiento: {X_train.shape[0]}; Features (palabras): {X_train.shape[1]}\")\n",
        "print(f\"Correos de Entrenamiento: {X_test.shape[0]}; Features (palabras): {X_test.shape[1]}\")\n",
        "print(\"Clases de Prueba:\")\n",
        "print(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhj0fGbdfnGP"
      },
      "source": [
        "## Entrenamiento - NAIVE BAYES MULTINOMIAL\n",
        "\n",
        "En el caso de un problema de clasificación de documentos, consideramos el vocabulario del conjunto de documentos. Cada documento es considerado como la secuencia del número de ocurrencias de cada palabra del vocabulario dentro del documento.\n",
        "\n",
        "\n",
        "Entonces, la distribución multinomial nos dice cuál es la probabilidad de que un documento pertenezca a una clase $y_j$.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "La distribución de probabilidad de la Variable Aleatoria $\\textrm{x}$ es una distribución multinomial (conteos de palabras) de la conjunción de palabras únicas, es decir del vocabulario, el cual es denotado por $V$.\n",
        "\n",
        "$P(x_i|y_j)$ nos dice qué tan probable es que la palabra $x_i$ aparezca en la clase $j$. Esta probabilidad está dada por:\n",
        "\n",
        "$$ P(x_i|y_j) = \\theta_i^j=\\frac{n_i + 1}{|S_j| + k} ;\\;\\;\\;\n",
        "\\begin{array}{l}\n",
        "i=1,2,\\ldots,k \\\\\n",
        "j\\in\\{\\text{Spam},\\text{Ham}\\}\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "* $|S_j|$ es la suma de los conteos de todas las palabras para la clase $y_j$. \n",
        "* $k=|V|$ es la longitud del vocabulario, el número de palabras únicas.\n",
        "* $n_i$ es la suma de los conteos para la palabra $i$ en la case $y_j$.\n",
        "\n",
        "Por la forma de calcular la probabilidad queremos evitar que $\\theta_i^j=0$.\n",
        "\n",
        "Al final obtenemos dos vectores $\\theta_0$ y $\\theta_1$:\n",
        "\n",
        "* $\\theta_0=(\\theta_1^{\\text{Spam}},\\ldots,\\theta_k^{\\text{Spam}})$, nos dice la probabilidad de que cada palabra aparezca en correos *Spam*.\n",
        "* $\\theta_1=(\\theta_1^{\\text{Ham}},\\ldots,\\theta_k^{\\text{Ham}})$, nos dice la probabilidad de que cada palabra aparezca en correos *Ham*.\n",
        "\n",
        "Observar que \n",
        "\n",
        "$$\\sum_{i=1}^k \\theta_i^{\\text{Ham}}=1,\\;\\;\\sum_{i=1}^k \\theta_i^{\\text{Spam}}=1$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fpk23FqIfnGQ"
      },
      "source": [
        "Entrena al modelo para obtener los vectores $\\theta_0$ y $\\theta_1$. utiliza la fórmula para $P(x_i|y_j)$.\n",
        "\n",
        "Crea una función que reciba un arreglo o un dataframe $X_{train}$ y devuelva dos vectores: $\\theta_p$ y $\\theta_n$."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(X_train,y_train):\n",
        "    \"\"\"\n",
        "    Entrenamiento del clasificador NAIVE BAYES:\n",
        "    Entrada: \n",
        "     - X_train: un DataFrame o Arreglo con instancias de entrenamiento.\n",
        "    Salida:\n",
        "     - una lista: [theta_n,theta_p] conteniendo \n",
        "       los vectores (np.arrays) de probabilidad de cada feature (palabra) \n",
        "       en correos Ham y Spam respectivamente. \n",
        "    \"\"\"\n",
        "    correos,k = X_train.shape # numero de palabras\n",
        "    print(f\"numero de palabras: {k}\")\n",
        "    print(f\"número de correos: {correos}\")\n",
        "    theta = []\n",
        "    for j in range(2):\n",
        "        x_train_clase = X_train[y_train==j]\n",
        "        n = np.sum(x_train_clase,axis=0)+1        # calcula n_i para cada palabra\n",
        "        S = np.sum(x_train_clase)+k               # calcula |S_j|+k\n",
        "        print(n)\n",
        "        theta.append(n/S)                 # calcula theta correspondiente\n",
        "    print('theta shapes=',theta[0].shape,theta[1].shape) #debe ser (k,) para ambos\n",
        "\n",
        "    return theta[0],theta[1]"
      ],
      "metadata": {
        "id": "jRZ_19v5s9lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVgGQ-UwfnGT"
      },
      "source": [
        "**Entrena el modelo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPU7jCLYfnGT"
      },
      "outputs": [],
      "source": [
        "theta_n, theta_p = fit(X_train,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "theta_n.shape, theta_p.shape"
      ],
      "metadata": {
        "id": "E2XR0nGBsjW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXAV4F_0fnGU"
      },
      "outputs": [],
      "source": [
        "print(\"La suma para vector debe ser 1 \\n Theta_p : \",np.sum(theta_p))\n",
        "print(\"La suma para vector debe ser 1 \\n Theta_n : \",np.sum(theta_n))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho04HflDfnGW"
      },
      "source": [
        "## Prueba del Modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yw1NO61yfnGW"
      },
      "source": [
        "La función que calcula la verosimilitud está dada por\n",
        "\n",
        "$$ p(\\textrm{x}|\\theta) =  \\prod_{i=1}^{k} p(x_i|\\theta) = n!\\times\\frac{\\theta_{1}^{x_1}}{x_1!}\\times...\\times\\frac{\\theta_k^{x_k}}{x_k!}$$\n",
        "\n",
        "donde $\\textrm{x}=(x_1,...,x_k)$ es el documento dado por los conteos de las palabras, $\\theta\\in\\{\\theta^\\text{Spam},\\theta^\\text{Ham}\\}$ y $n=\\sum x_i$.\n",
        "\n",
        "La función de verosimilitud nos dice qué tanto una clase $j$ explica (hace creíbles) los datos $\\textrm{x}$. \n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "La implementación de la distribución de probabilidad de arriba puede llevar a errores numéricos por términos $x_i$ muy grandes o $\\theta_i$ muy pequeños. Por lo tanto calculamos la *verosimilitud logarítmica*:\n",
        "\n",
        "$$ \\log(p(\\textrm{x}|\\theta)) =  \\log(n!) + x_1\\log(\\theta_{1})-\\log(x_1!)+ ... + x_k\\log(\\theta_{k})-\\log(x_k!)$$\n",
        "\n",
        "Después usamos la exponencial para obtener $p(\\textrm{x}|\\theta)$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxr0hMmXfnGW"
      },
      "source": [
        "\n",
        "* Se recomienda usar:\n",
        "  - `np.math.log` para escalares\n",
        "  - `np.log` para arreglos\n",
        "  - `np.math.factorial` para escalares \n",
        "  - `scipy.special.factorial` para arreglos.\n",
        "  - `np.math.exp` como inverso de `log`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMuSdlm2fnGX"
      },
      "outputs": [],
      "source": [
        "from scipy.special import factorial\n",
        "\n",
        "def P_Mi(x,theta):\n",
        "    \"\"\"\n",
        "    Cálcula la función de verosimilitud: P(x|theta)\n",
        "    Entrada:\n",
        "     - x: arreglo unidimensional de datos\n",
        "     - theta: arreglo unidimensional de probabilidades dim(theta)=dim(x)\n",
        "    Salida:\n",
        "     - Valor de log(P(x|theta)) según distribución MULTINOMIAL de x\n",
        "    \"\"\"\n",
        "    if not isinstance(x,np.ndarray):  \n",
        "        raise TypeError('Error, x debe ser de tipo numpy.ndarray')\n",
        "        \n",
        "    P_x = 0\n",
        "    \n",
        "    '''\n",
        "    SOLUCIÓN 1: USANDO UN CICLO 'FOR'\n",
        "    '''  \n",
        "    # prod = 0   \n",
        "    # n_i = 0  \n",
        "    # for i in range(len(x)):\n",
        "    #     n_i += x[i]\n",
        "    #     prod += (x[i]*np.math.log(theta[i]) - np.math.log(np.math.factorial(x[i])))\n",
        "    # n_fact = np.math.log(np.math.factorial(n_i))\n",
        "    # P_x = prod + n_fact\n",
        "\n",
        "    '''\n",
        "    SOLUCIÓN 2: USANDO VECTORIZACIÓN DE NUMPY (MÁS RÁPIDO)\n",
        "    '''  \n",
        "    P_x = x*np.log(theta)-np.log(factorial(x))\n",
        "    P_x = np.sum(P_x)\n",
        "    n = np.sum(x)\n",
        "    P_x += np.math.log(np.math.factorial(n))\n",
        "    \"\"\" ------------- \"\"\"\n",
        "    \n",
        "    return np.math.exp(P_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IslQiBi1fnGX"
      },
      "source": [
        "Definimos la función que clasifica X mediante el criterio LR:<br> <br>\n",
        "\n",
        "$$ LR = \\frac{p(\\textrm{x}|\\theta_{Pos})}{p(\\textrm{x}|\\theta_{Neg})}$$\n",
        "\n",
        "\n",
        "Este modelo utiliza la razón de verosimilitud (LR) para hacer la predición de la clase del documento $\\textrm{x}$.\n",
        "\n",
        "* Si $LR > 1$ $\\Rightarrow$ $y=1$ <br>\n",
        "* Si $LR < 1$ $\\Rightarrow$ $y=0$ <br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdJTvdXNfnGX"
      },
      "outputs": [],
      "source": [
        "def clasifica_LR(X_test,theta_p, theta_n):\n",
        "    \"\"\"\n",
        "    Calcula: LR=P(x|y=+)/P(x|y=-)\n",
        "    Entrada:\n",
        "     - X_test: arreglo o DataFrame de datos\n",
        "     - theta_p: arreglo unidimensional de probabilidades Pos\n",
        "     - theta_n: arreglo unidimensional de probabilidades Neg\n",
        "    Salida:\n",
        "     - Valor de LR\n",
        "    \"\"\"\n",
        "    instancias = X_test.shape[0]\n",
        "    y = np.zeros(instancias)\n",
        "    if not isinstance(X_test,np.ndarray):\n",
        "        X_ = X_test.values\n",
        "    else:\n",
        "        X_ = X_test \n",
        "    for i in range(instancias):\n",
        "        pos = P_Mi(X_[i,:],theta_p)\n",
        "        neg = P_Mi(X_[i,:],theta_n)\n",
        "        try:\n",
        "            lr = pos/neg\n",
        "        except:\n",
        "            lr = 1\n",
        "        if lr>=1:\n",
        "            y[i] = 1\n",
        "        else:\n",
        "            y[i] = 0\n",
        "    return y.astype(int)    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrdHLW7JfnGY"
      },
      "source": [
        "## Evaluamos el modelo\n",
        "\n",
        "La siguiente función predice las etiquetas de un conjunto de prueba.\n",
        "\n",
        "Recibe el arreglo de datos de prueba, el vector theta positivo y negativo para el LR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqjoRTDOfnGY"
      },
      "outputs": [],
      "source": [
        "y_predic = clasifica_LR(X_test,theta_p, theta_n)\n",
        "print(y_predic)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluamos la tarea de predicción usando las métricas de desempeño y la matriz de confusión"
      ],
      "metadata": {
        "id": "AYg3q1voUVm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "\n",
        "print(f\"Accuracy: {accuracy_score(y_test,y_predic)}\")\n",
        "print(f\"Recall: {recall_score(y_test,y_predic)}\")\n",
        "print(f\"Precision: {precision_score(y_test,y_predic)}\")"
      ],
      "metadata": {
        "id": "-azfMC9W1_mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test,y_predic)\n",
        "cm"
      ],
      "metadata": {
        "id": "NTuw0IE3zNmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(3,3))\n",
        "sns.heatmap(cm,cmap='plasma',annot=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YQC6Cs8fzkRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⭕ Práctica\n",
        "\n",
        "Trabajaremos con el corpus `20newsgroups`. Este es un conjunto de documentos conteniendo cerca de 18000 posts perteneciendo a 20 temas. Tomaremos documentos pertenecientes a 2 temas (*autos* y *ateísmo*). Usaremos nuestro clasificador y la implementación del Naive-Bayes de scikit-learn para hacer la clasificación. "
      ],
      "metadata": {
        "id": "MLZoNT8VLxZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos el módulo `nltk`, el cual contiene herramientas para el PLN."
      ],
      "metadata": {
        "id": "NSReSGzQVQSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "E4QsiRZsVrAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bajamos la colección de signos de puntuación y *stopwords* para hacer el preprocesamiento. Las stopwords (palabras vacías) son palabras sin significado como artículos, pronombres, preposiciones, etc. que son filtradas antes o después del procesamiento de datos en lenguaje natural."
      ],
      "metadata": {
        "id": "LwT0QKfKoWRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import download\n",
        "\n",
        "download('stopwords')\n",
        "download('punkt')"
      ],
      "metadata": {
        "id": "HqKDM17fWMXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bajamos el conjunto de documentos"
      ],
      "metadata": {
        "id": "hKzwY4AqoilA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "(Docs,Y) = fetch_20newsgroups(\n",
        "                        categories=['rec.autos','alt.atheism'],\n",
        "                        remove=('headers', 'footers', 'quotes'),\n",
        "                        return_X_y=True\n",
        "                        )"
      ],
      "metadata": {
        "id": "G3rmEshZLy2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vista previa de los primeros 3 documentos"
      ],
      "metadata": {
        "id": "E5r8FsQwooff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Docs[:3]"
      ],
      "metadata": {
        "id": "us40sWLdTV1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "id": "T2EgoxCFvsW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Función para preprocesar el texto, limpiarlo y quitar *stopwords*. \n",
        "\n",
        "Esta función usa expresiones regulares, las cuales son muy útiles en el tratamiento de texto. Más información: https://regex101.com/\n",
        "\n"
      ],
      "metadata": {
        "id": "tOiWiRMdoCf1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "\n",
        "def preprocesar_textos(docs,ignore_list:list=[]):\n",
        "    strings_list = [re.sub(r'\\n', '', x.lower()) for x in docs]\n",
        "    strings_list = [re.sub(r'[^\\w\\s]', '', x.lower()) for x in docs]  # quita signos de puntuación\n",
        "    strings_list = [re.sub('[0-9]', '', x.lower()) for x in strings_list] # quita números\n",
        "    SW = stopwords.words('english') \n",
        "    for x in ignore_list: # quitamos stopwords\n",
        "        SW.remove(x)\n",
        "    tokens_no_sw = [\" \".join([word for word in word_tokenize(text) if not word in SW]) for\n",
        "                         text in strings_list ]\n",
        "    return tokens_no_sw"
      ],
      "metadata": {
        "id": "Q7rPpOmbabrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vista previa de los primeros 3 documentos ya preprocesados"
      ],
      "metadata": {
        "id": "LzoVj3xgn9LN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documentos = preprocesar_textos(Docs)\n",
        "documentos[:3]"
      ],
      "metadata": {
        "id": "5HZoiotVarMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`CountVectorizer` crea la matriz de términos-documentos necesaria para aplicar nuestro Naive-Bayes multinomial.\n",
        "\n",
        "** Prueba cambiando el parámetro `max_features`"
      ],
      "metadata": {
        "id": "lKwU732Hop9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vec = CountVectorizer(max_features=8000)\n",
        "X = vec.fit_transform(documentos)\n",
        "\n",
        "X = np.array(X.todense())\n",
        "\n",
        "print(X.shape)"
      ],
      "metadata": {
        "id": "Uq8G3EvIa4v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Diccionario que contiene el índice de cada palabra en el vocabulario"
      ],
      "metadata": {
        "id": "QwPPU-3roz2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vec.vocabulary_"
      ],
      "metadata": {
        "id": "hzOzgYAgjIiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separamos en train/test"
      ],
      "metadata": {
        "id": "LKfR5Ovto56b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y,train_size=0.8,random_state=9)"
      ],
      "metadata": {
        "id": "5e5JjbDJgB8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando nuestro clasificador"
      ],
      "metadata": {
        "id": "S0B2s1jU0Fqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recupera la función de entrenamiento implementada en el ejercicio de SPAM. Es decir, copiala aquí y modificala si es necesario."
      ],
      "metadata": {
        "id": "-pLezLAi1pGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(X_train,y_train):\n",
        "    \n",
        "    '''\n",
        "    llena el código\n",
        "    '''\n",
        "\n",
        "    return theta[0],theta[1]"
      ],
      "metadata": {
        "id": "eZBMZqE-0FAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamos el modelo"
      ],
      "metadata": {
        "id": "GVrKk4Zbo9A3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "theta_n, theta_p = fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "JOyU9KZSggRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"La suma para vector debe ser 1 \\n Theta_p : {np.sum(theta_p)}\")\n",
        "print(f\"La suma para vector debe ser 1 \\n Theta_n : {np.sum(theta_n)}\")"
      ],
      "metadata": {
        "id": "5doS5Go20jvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtenemos el vector de predicciones"
      ],
      "metadata": {
        "id": "zkf25E50pFSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_predic = clasifica_LR(X_test,theta_p, theta_n)\n",
        "print(y_predic)"
      ],
      "metadata": {
        "id": "vxDpi8zL10gK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizamos los resultados y las métricas"
      ],
      "metadata": {
        "id": "BMreyGrlpHzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test,y_predic)\n",
        "cm"
      ],
      "metadata": {
        "id": "34GRcNeV14fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "\n",
        "print(f\"Accuracy: {accuracy_score(y_test,y_predic)}\")\n",
        "print(f\"Recall: {recall_score(y_test,y_predic)}\")\n",
        "print(f\"Precision: {precision_score(y_test,y_predic)}\")"
      ],
      "metadata": {
        "id": "r_-mw6TH2G8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usando sklearn"
      ],
      "metadata": {
        "id": "qL2ndvSPaHoM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usemos la implementación del clasificador Naive-Bayes de scikit-learn: \n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
        "\n",
        "Realiza el entrenamiento y obten las predicciones para el conjunto de prueba"
      ],
      "metadata": {
        "id": "tNCBfH_i2rDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import numpy as np\n",
        "\n",
        "clf_nb = MultinomialNB()\n",
        "clf_nb.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "f269TEDPVodY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obten las métricas de rendimiento y la matriz de confusión."
      ],
      "metadata": {
        "id": "685dxiEzxDhd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mmP6zTzXZ-XC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}