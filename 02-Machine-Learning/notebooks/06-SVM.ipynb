{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DCDPUAEM/DCDP/blob/main/02-Machine-Learning/notebooks/06-SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM\n",
        "\n",
        "En esta notebook mostraremos el uso del clasificador **SVM** (Support Vector Machine). Realizaremos un ejemplo con datos artificiales, con fines didácticos, y un ejemplo más grande, con datos reales.\n",
        "\n",
        "Usaremos la implementación de sklearn, llamada [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) (Support Vector Classifier)"
      ],
      "metadata": {
        "id": "vmWhZfIxi6yU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 1"
      ],
      "metadata": {
        "id": "UlqlbDHkTIsl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glVe2E6ySpTP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Funciones que necesitamos para graficar las fronteras de decisión"
      ],
      "metadata": {
        "id": "8cxOlJX_il7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def make_meshgrid(x, y, h=.02):\n",
        "    x_min, x_max = x.min() - 1, x.max() + 1\n",
        "    y_min, y_max = y.min() - 1, y.max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "    return xx, yy\n",
        "\n",
        "def plot_contours(ax, clf, xx, yy, **params):\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    out = ax.contourf(xx, yy, Z, **params)\n",
        "    return out"
      ],
      "metadata": {
        "id": "DFox_ShDikzl",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### El conjunto de datos"
      ],
      "metadata": {
        "id": "awsozxk0i4e9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos un conjunto de datos con una condición XOR"
      ],
      "metadata": {
        "id": "iX00M-qDi1zU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDNOtk2FTRDv"
      },
      "outputs": [],
      "source": [
        "X = np.random.randn(1000, 2)\n",
        "Y = np.array([int(np.logical_xor(x[0] > 0, x[1] > 0)) for x in X])\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X[Y==0, 0], X[Y==0, 1], s=10, label='0')\n",
        "plt.scatter(X[Y==1, 0], X[Y==1, 1], s=10, label='1')\n",
        "plt.legend()\n",
        "plt.xlabel('x1',fontsize=16)\n",
        "plt.ylabel('x2',fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separamos el conjunto de datos en train y test."
      ],
      "metadata": {
        "id": "Jy64NizLi95C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7-Hi2BmTUrd",
        "outputId": "19f5e741-90fd-4de4-f898-02fc5393cd9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X Train: (800, 2)\n",
            "X Test: (200, 2)\n",
            "Y Train: (800,)\n",
            "Y Test: (200,)\n"
          ]
        }
      ],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
        "\n",
        "print(f\"X Train: {x_train.shape}\")\n",
        "print(f\"X Test: {x_test.shape}\")\n",
        "print(f\"Y Train: {y_train.shape}\")\n",
        "print(f\"Y Test: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEmnVOw6XJqq"
      },
      "source": [
        "### Clasificación"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### SVM lineal"
      ],
      "metadata": {
        "id": "MLnixjtrjGoE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt_JmIokV0xU",
        "outputId": "83b8b11b-4a27-4232-930b-137bfa9eebc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training mean accuracy: 0.618\n",
            "Test mean accuracy: 0.635\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "lin_svm = SVC(kernel='linear')\n",
        "lin_svm.fit(x_train, y_train)\n",
        "\n",
        "# Performance\n",
        "print(f\"Training mean accuracy: {round(lin_svm.score(x_train, y_train),3)}\")\n",
        "print(f\"Test mean accuracy: {round(lin_svm.score(x_test, y_test),3)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xx, yy = make_meshgrid(X[:,0], X[:,1]) # Hacemos el grid para graficar las regiones\n",
        "\n",
        "fig, ax = plt.subplots(dpi=100)  # El parámetro dpi especifíca los puntos por pulgada (DPI) de la imagen\n",
        "plot_contours(ax, lin_svm, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "ax.scatter(X[:,0], X[:,1], c=Y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
        "ax.set_ylabel('x2', fontsize=16)\n",
        "ax.set_xlabel('x1', fontsize=16)\n",
        "ax.set_xticks(())\n",
        "ax.set_yticks(())\n",
        "ax.set_title('Frontera de decisión del SVM')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m7S5VurHjcs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ⭕ Probar otros kernels"
      ],
      "metadata": {
        "id": "EGMoIZhmsUbQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Documentación: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html"
      ],
      "metadata": {
        "id": "Yl7aeHt7sm1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "1. Repetir el experimento de clasificación de arriba, usando otros kernels.\n",
        "2. Graficar\n",
        "3. ¿Qué kernel parece dar mejor resultado?\n",
        "'''"
      ],
      "metadata": {
        "id": "jY4JoakxsULC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "7bf715ec-071d-4fcb-a183-6cb8c9d5570c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n1. Repetir el experimento de clasificación de arriba, usando otros kernels.\\n2. Graficar\\n3. ¿Qué kernel parece dar mejor resultado?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* El kernel lineal es mejor para datos linealmente separables. Es una opción cuando el conjunto de datos es grande. \n",
        "* El kernel Gaussiano (RBF) tiende a dar buenos resultados cuando no se tiene información adicional sobre los datos.\n",
        "* Los kernels polinomiales tienden a dar buenos resultados cuando los datos de entrenamiento están normalizados."
      ],
      "metadata": {
        "id": "Vqs84jSTs43l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usando gridsearch para encontrar los mejores parámetros"
      ],
      "metadata": {
        "id": "2AFz2eTyjT3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) toma un estimador (por ejemplo, SVM) y un conjunto de parámetros del estimador. Sobre estos parámetros hace una busqueda para encontrar la combinación de parámetros que da mejores resultados en el estimador. \n",
        "\n",
        "GridSearchCV tiene métodos “fit” y “score” method, entre otros. Es decir, no es necesario tomar los parámetros e introducirlos en el estimador."
      ],
      "metadata": {
        "id": "fC23ZQAg7EpH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encuentra los mejores parámetros para el clasificador SVM utilizando grid search. Guíate por el desempeño en el set de entrenamiento y validación.\n",
        "\n",
        "Prueba los siguientes hyperparámetros.\n",
        "* kernel = linear, polynomial, rbf\n",
        "* C = 0.01, 0.1, 1.0, 10, 100\n",
        "* grado del polinomio = 1, 2, 3, 4 (solo para el kernel polinomial)\n",
        "* gamma = auto, scale:"
      ],
      "metadata": {
        "id": "5ymSFVzkcf03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos los parámetros sobre los que se hará la busqueda"
      ],
      "metadata": {
        "id": "CJZKn4kxo7LH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fehVJTIZZEmP"
      },
      "outputs": [],
      "source": [
        "param_grid = {'C': [0.01, 0.1, 1, 10, 100], 'kernel': ('linear', 'poly', 'rbf'),\n",
        "              'degree': [1, 2, 3, 4], 'gamma': ('auto', 'scale')}\n",
        "param_grid"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizamos una busqueda sobre estos parámetros "
      ],
      "metadata": {
        "id": "j6-yzMJUI40z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_mFhIYkaG_y"
      },
      "outputs": [],
      "source": [
        "svc = SVC()\n",
        "clf = GridSearchCV(svc, param_grid)\n",
        "clf.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgCYpURZbCH2",
        "outputId": "dd61ab06-c4e1-431c-90c0-06a3f3661031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best score: 0.9900\n",
            "Best params: {'C': 100, 'degree': 2, 'gamma': 'auto', 'kernel': 'poly'}\n"
          ]
        }
      ],
      "source": [
        "# Print info about best score and best hyperparameters\n",
        "print(f\"Best score: {clf.best_score_:.4f}\")\n",
        "print(f\"Best params: {clf.best_params_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jclaEUreczue",
        "outputId": "8439e951-11b1-4484-eb2c-a857a5f2005a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train mean accuracy: 0.9862\n",
            "Test mean accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on the test set\n",
        "best_svm = SVC(C=100, kernel='poly', degree=2, gamma='auto')\n",
        "best_svm.fit(x_train, y_train)\n",
        "\n",
        "print(f\"Train mean accuracy: {best_svm.score(x_train, y_train):6.4f}\")\n",
        "print(f\"Test mean accuracy: {best_svm.score(x_test, y_test):6.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Graficamos la frontera de decisión"
      ],
      "metadata": {
        "id": "cyE6AWhdh1OV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTSvVYqTdEwM"
      },
      "outputs": [],
      "source": [
        "xx, yy = make_meshgrid(X[:,0], X[:,1]) # Hacemos el grid para graficar las regiones\n",
        "\n",
        "fig, ax = plt.subplots(dpi=100)  # El parámetro dpi especifíca los puntos por pulgada (DPI) de la imagen\n",
        "plot_contours(ax, best_svm, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "ax.scatter(X[:,0], X[:,1], c=Y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
        "ax.set_ylabel('x2', fontsize=16)\n",
        "ax.set_xlabel('x1', fontsize=16)\n",
        "ax.set_xticks(())\n",
        "ax.set_yticks(())\n",
        "ax.set_title('Frontera de decisión del SVM')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparando el SVM lineal con el OLS (clasificador lineal)"
      ],
      "metadata": {
        "id": "fyhJ9XBnZCTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este ejercicio vamos a comparar la clasificación y la frontera de decisión del clasificador de la sesión anterior (discriminante lineal OLS) con el SVM con kernel lineal.\n",
        "\n",
        "Para esto, vamos a usar ambos clasificadores en el mismo conjunto de datos. Después, compararemos la frontera de decisión."
      ],
      "metadata": {
        "id": "GyBLYOZ5ZKb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado que el clasificador lo implementamos como una clase, podemos usarlo en esta notebook directamente. Hay dos maneras de hacerlo:\n",
        "\n",
        "* Copiando el código y definiendo la clase otra vez:"
      ],
      "metadata": {
        "id": "qhjvxInbCZdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "class LeastSquaresClassifier():\n",
        "    def __init__(self, W:np.ndarray=None):\n",
        "        '''\n",
        "        W es la matriz de pesos, la cual puede ser especificada desde un principio, esto\n",
        "        es opcional.\n",
        "        '''\n",
        "        self.W = W\n",
        "\n",
        "    def encoderT(self, y:np.ndarray):\n",
        "        K = np.max(y) + 1\n",
        "        identidad = np.eye(K)\n",
        "        return identidad[y] \n",
        "\n",
        "    def fit(self, X:np.ndarray, y:np.ndarray):\n",
        "        '''\n",
        "        Este método calcula la matriz de pesos para la matriz de puntos \"aumentada\" X\n",
        "        y el conjunto de etiquetas y.\n",
        "        '''\n",
        "        T = self.encoderT(y)\n",
        "        self.W = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ T\n",
        "        \n",
        "    def clasifica(self, X:np.ndarray):\n",
        "        '''\n",
        "        Este método predice las etiquetas para el conjunto de puntos X\n",
        "        '''\n",
        "        return np.argmax(X@self.W,axis=1)"
      ],
      "metadata": {
        "id": "m-z82RYuZxR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Importando la clase desde un archivo:"
      ],
      "metadata": {
        "id": "P72hCNryCnxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from clasificador_lineal import LeastSquaresClassifier"
      ],
      "metadata": {
        "id": "96ypeu4tZiYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos el conjunto de datos, usaremos un dataset de scikit-learn:"
      ],
      "metadata": {
        "id": "-dxo2D_2C-_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_moons\n",
        "\n",
        "x_train, y_train = make_moons(n_samples = 120, random_state=89,noise=0.1)\n",
        "\n",
        "#--- Lo graficamos para verlo ---\n",
        "plt.figure()\n",
        "plt.scatter(x_train[:,0], x_train[:,1], c=y_train)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "35_M3k2NEbua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⭕ Realiza la clasificación usando el clasificador OLS y grafica la frontera de decisión.\n",
        "\n",
        "Puedes usar el código para clasificar y graficar que usamos en la sesión anterior"
      ],
      "metadata": {
        "id": "fvRzxc_GMfO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "x1_test, x2_test = np.meshgrid(np.linspace(-2, 2.5, 100), np.linspace(-1, 1.5, 100))\n",
        "x_test = np.array([x1_test, x2_test]).reshape(2, -1).T\n",
        "\n",
        "features = PolynomialFeatures(1)\n",
        "X_train = features.fit_transform(x_train)\n",
        "X_test = features.fit_transform(x_test)\n",
        "\n",
        "#------ COMPLETAR ------\n",
        "\n",
        "#----------------------\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train)\n",
        "plt.contour(x1_test, x2_test, y_ols.reshape(100, 100))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qbcn1VCtC4Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⭕ Ahora, usemos SVM lineal\n",
        "\n",
        "Realiza la clasificación en el mismo dataset, usando SVM con kernel lineal y grafica la frontera de decisión. Puedes usar el código para clasificar y graficar que usamos anteriormente."
      ],
      "metadata": {
        "id": "vv8GUSe4NTzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1_test, x2_test = np.meshgrid(np.linspace(-2, 2.5, 100), np.linspace(-1, 1.5, 100))\n",
        "x_test = np.array([x1_test, x2_test]).reshape(2, -1).T\n",
        "\n",
        "#------ COMPLETAR ------\n",
        "\n",
        "#----------------------\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train)\n",
        "plt.contour(x1_test, x2_test, y_svm.reshape(100, 100))\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4BjC4BY7KHud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dibujamos ambas FD juntas. "
      ],
      "metadata": {
        "id": "9OeJaZ81enO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "#-----Dibujar los datos---------------------------------\n",
        "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train)\n",
        "#-----Dibujar X_test (la malla de fondo para ver las regiones ------------\n",
        "plt.contour(x1_test, x2_test, y_ols.reshape(100, 100),colors='green')\n",
        "plt.contour(x1_test, x2_test, y_svm.reshape(100, 100),colors='blue')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b1AUz8FuXeMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observar que no son la misma.\n",
        "\n",
        "🔵 ¿Por qué no?"
      ],
      "metadata": {
        "id": "I5CLQPSEPmmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 2"
      ],
      "metadata": {
        "id": "QlT0ezfs_PwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este problema usaremos el datset de Kaggle: [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud)\n",
        "\n",
        "**Contexto**\n",
        "\n",
        "Los conjuntos de datos contienen transacciones realizadas con tarjetas de crédito en septiembre de 2013 por titulares de tarjetas europeos. Este conjunto de datos presenta transacciones que ocurrieron en dos días, donde tenemos 492 fraudes de 284,807 transacciones. El conjunto de datos está altamente desequilibrado, la clase positiva (fraudes) representa el 0.172% de todas las transacciones.\n",
        "\n",
        "Contiene solo variables de entrada numéricas que son el resultado de una transformación PCA. Desafortunadamente, debido a problemas de confidencialidad, no se pueden obtener las características originales y más información de fondo sobre los datos. Las características $V_1$, $V_2$, ..., $V_{28}$ son los componentes principales obtenidos con PCA, las únicas características que no se han transformado con PCA son 'Tiempo' y 'Cantidad'. La función 'Tiempo' contiene los segundos transcurridos entre cada transacción y la primera transacción en el conjunto de datos. La característica 'Cantidad' es la Cantidad de la transacción, esta característica se puede utilizar para el aprendizaje sensible al costo dependiente del ejemplo. La característica 'Clase' es la variable de respuesta y toma el valor 1 en caso de fraude y 0 en caso contrario."
      ],
      "metadata": {
        "id": "-cCQM40V_dJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -qq install > /dev/null subversion\n",
        "\n",
        "!svn checkout \"https://github.com/DCDPUAEM/DCDP/trunk/02-Machine-Learning/data/\""
      ],
      "metadata": {
        "id": "oM1KK1hoUr1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extraer el archivo zip"
      ],
      "metadata": {
        "id": "WQNavL28_3jK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.options.mode.chained_assignment = None\n",
        "\n",
        "from zipfile import ZipFile \n",
        "\n",
        "archivo = \"/content/data/creditcard.zip\"\n",
        "\n",
        "print('Extrayendo contenido...') \n",
        "with ZipFile(archivo, 'r') as Zip: \n",
        "    Zip.extractall() \n",
        "    print('Extracción finalizada.') \n",
        "\n",
        "credito = pd.read_csv(\"creditcard.csv\")"
      ],
      "metadata": {
        "id": "8AGYagMwh8GI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credito.head()"
      ],
      "metadata": {
        "id": "60RwWl3OAM7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "credito.describe()"
      ],
      "metadata": {
        "id": "xb2G0chpAP-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "\n",
        "# Graficamos los que no son fraude\n",
        "time_amount = credito[credito['Class'] == 0][['Time','Amount']].values\n",
        "plt.scatter(time_amount[:,0], time_amount[:,1], \n",
        "            c='green',alpha=0.25,label='Clean')\n",
        "# Graficamos los que sí son fraude\n",
        "time_amount = credito[credito['Class'] == 1][['Time','Amount']].values\n",
        "plt.scatter(time_amount[:,0], time_amount[:,1], \n",
        "            c='red',label='Fraud')\n",
        "plt.legend(loc='best')\n",
        "plt.title('Amount vs fraud')\n",
        "plt.xlabel('Time', fontsize=16)\n",
        "plt.ylabel('Amount', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dId7E_NrAXAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "sns.countplot(x = \"Class\", data = credito)\n",
        "plt.show()\n",
        "\n",
        "credito.loc[:, 'Class'].value_counts()"
      ],
      "metadata": {
        "id": "rHQqW8EqAbW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "No_of_frauds= len(credito[credito[\"Class\"]==1])\n",
        "No_of_normals = len(credito[credito[\"Class\"]==0])\n",
        "print(\"Hay {} transacciones fraudulentas ( Class 1)\".format(No_of_frauds))\n",
        "print(\"Hay {} transacciones normales ( Class 0)\".format(No_of_normals))\n",
        "total= No_of_frauds + No_of_normals\n",
        "pf= (No_of_frauds / total)*100\n",
        "pn= (No_of_normals / total)*100\n",
        "print(\"Porcentaje clase 0 = {}%\".format(np.round(pn,2)))\n",
        "print(\"Porcentaje clase 1 = {}%\".format(np.round(pf,2)))"
      ],
      "metadata": {
        "id": "OaWkLhQvBLKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFExW0e-20l7"
      },
      "source": [
        "### Submuestro\n",
        "\n",
        "Se necesita hacer un submuestreo para balancear las clases\n",
        "\n",
        "* Está claro que la Clase 1 está subrepresentada ya que solo  representa el 0.17% de todo el conjunto de datos. \n",
        "* Si entrenamos nuestro modelo usando este conjunto de datos, el modelo será ineficiente y será entrenado para predecir solo la Clase 0 porque no tendrá suficientes datos de entrenamiento.\n",
        "* Podemos obtener una alta exactitud al probar el modelo, pero no debemos confundirnos con esto porque nuestro conjunto de datos no tiene datos de prueba equilibrados. Por lo tanto, tenemos que confiar en el recall que se basa en TP y FP.\n",
        "* En los casos en que tengamos datos asimétricos, agregar datos adicionales de la característica subrepresentada (sobremuestreo) es una opción, mediante la modelación de la distribución de los datos. Por ahora no tenemos esa opción, así que tendremos que recurrir al submuestreo.\n",
        "* El submuestreo del conjunto de datos implica mantener todos nuestros datos subrepresentados (Clase 1) mientras se muestrea el mismo número de características de la Clase 0 para crear un nuevo conjunto de datos que comprenda una representación igual de ambas clases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MucsJV0r20l7"
      },
      "source": [
        "Obtenemos un conjunto de datos más balanceado que contenga el doble de instancias no fraudulentas respecto a las fraudulentas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4OPAV8M20l8"
      },
      "outputs": [],
      "source": [
        "#lista los indices de fraude\n",
        "fraud_idxs = credito[credito[\"Class\"]==1].index.to_list()\n",
        "\n",
        "#lista de indices normales del data set completo\n",
        "normal_idxs = credito[credito[\"Class\"]==0].index.to_list()\n",
        "\n",
        "#seleccion del numero de indices aleatorias igual al de transacciones fraudulentas\n",
        "random_normal_idxs = np.random.choice(normal_idxs, No_of_frauds*2, replace= False)\n",
        "random_normal_idxs = np.array(random_normal_idxs)\n",
        "\n",
        "#concatena indices fraudulentos y normales para tener una lista de indices\n",
        "undersampled_indices = np.concatenate([fraud_idxs, random_normal_idxs])\n",
        "\n",
        "#usa la lista de indices sub-muestreados para obtener el data frame\n",
        "undersampled_data = credito.iloc[undersampled_indices, :]\n",
        "\n",
        "print(f\"Fraude: {len(fraud_idxs)}, Normales: {len(random_normal_idxs)}\")\n",
        "\n",
        "undersampled_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6Wrm6M620l8"
      },
      "source": [
        "Comprobemos que los datos quedaron balanceados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TRLcfkS20l9"
      },
      "outputs": [],
      "source": [
        "No_of_frauds_sampled = len(undersampled_data[undersampled_data[\"Class\"]== 1])\n",
        "\n",
        "No_of_normals_sampled = len(undersampled_data[undersampled_data[\"Class\"]== 0])\n",
        "\n",
        "print(\"Número de transacciones fraudulentas (clase 1): \", No_of_frauds_sampled)\n",
        "print(\"Número de transacciones normales (clase 0): \", No_of_normals_sampled)\n",
        "total_sampled= No_of_frauds_sampled + No_of_normals_sampled\n",
        "print(\"Número total de instancias: \", total_sampled)\n",
        "\n",
        "Fraud_percent_sampled = (No_of_frauds_sampled / total_sampled)*100\n",
        "Normal_percent_sampled = (No_of_normals_sampled / total_sampled)*100\n",
        "print(\"Porcentaje clase 0 = \", Normal_percent_sampled)\n",
        "print(\"Porcentaje clase 1 = \", Fraud_percent_sampled)\n",
        "\n",
        "\n",
        "count_sampled = pd.value_counts(undersampled_data[\"Class\"], sort= True)\n",
        "count_sampled.plot(kind= 'bar')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_PjO_td20l9"
      },
      "source": [
        "### Oversampling\n",
        "\n",
        "Ahora haremos un proceso llamado [SMOTE: Synthetic Minority Over-sampling Technique](https://arxiv.org/abs/1106.1813)\n",
        "\n",
        "\n",
        "Para ello necesitamos instalar la librería de _aprendizaje desequilibrado_ ``imbalanced-learn`` de Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa_Vb4NP20l_"
      },
      "outputs": [],
      "source": [
        "!pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4fPH1Bd20mA"
      },
      "outputs": [],
      "source": [
        "import imblearn\n",
        "print(imblearn.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTSJsp2K20mB"
      },
      "source": [
        "Re-escalemos los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqtPP2k520mB"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "sc = preprocessing.StandardScaler()\n",
        "\n",
        "undersampled_data.loc[:,\"scaled_Amount\"] = sc.fit_transform(undersampled_data[\"Amount\"].values.reshape(-1,1))\n",
        "\n",
        "# quitamos las columnas \"Time\" y \"Amount\"\n",
        "undersampled_data.drop([\"Time\",\"Amount\"], axis= 1,inplace=True)\n",
        "\n",
        "undersampled_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaNpF9Cu20mB"
      },
      "source": [
        "⭕ Obtén la matriz de datos $X$ y el vector de clases $y$ correspondiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fWtfSkU20mB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF4zrMIG20mC"
      },
      "source": [
        "Hagamos el proceso de sobre-muestreo [SMOTE](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfmmO0By20mC"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "oversample = SMOTE()\n",
        "X, y = oversample.fit_resample(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG7Hf5ix20mD"
      },
      "source": [
        "Verifiquemos la cantidad de datos ahora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDdO9Cyp20mD"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "counter = Counter(y)\n",
        "print(counter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sb2gdqXw20mH"
      },
      "source": [
        "### Crear el conjunto de entrenamiento y prueba\n",
        "\n",
        "Separa los datos en datos de entrenamiento (75%) y prueba (25%) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3loC9tl20mH"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25, random_state= 0)\n",
        "print(\"X_train: \", len(X_train))\n",
        "print(\"X_test: \", len(X_test))\n",
        "print(\"y_train: \", len(y_train))\n",
        "print(\"y_test: \", len(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5GqWW6Q20mH"
      },
      "source": [
        "⭕ Elige una SVM y entrénalo con un conjunto de parámetros de tu elección "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekwgZfiG20mH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp_YhOUu20mI"
      },
      "source": [
        "### Prueba el modelo \n",
        "\n",
        "Realiza las predicciones con el conjunto de prueba y bserva la matriz de confusión."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTRsqljv20mI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "CM = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(CM)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "También podemos calcular las métricas de rendimiento *manualmente*."
      ],
      "metadata": {
        "id": "2wgQFPGzZMnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The accuracy is \"+str((CM[1,1]+CM[0,0])/(CM[0,0] + CM[0,1]+CM[1,0] + CM[1,1])*100) + \" %\")\n",
        "print(\"The recall from the confusion matrix is \"+ str(CM[1,1]/(CM[1,0] + CM[1,1])*100) +\" %\")"
      ],
      "metadata": {
        "id": "5bYk4F11ZMVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⭕ Calcula también el *F1-score* y el *precision score*"
      ],
      "metadata": {
        "id": "WGQVuXnXZdHd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b9sl11tpbkdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRmyrMl720mK"
      },
      "source": [
        "### Aplica GridSearch para obtener los mejores parámetros para una SVM "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJEOf9L320mK"
      },
      "outputs": [],
      "source": [
        "parameters = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
        "              {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\n",
        "\n",
        "grid_search = GridSearchCV(estimator = classifier,\n",
        "                           param_grid = parameters,\n",
        "                           scoring = 'accuracy',\n",
        "                           cv = 5,\n",
        "                           n_jobs = -1)\n",
        "\n",
        "grid_search = grid_search.fit(X_train, y_train.ravel())\n",
        "best_accuracy = grid_search.best_score_\n",
        "print(\"The best accuracy using gridSearch is\", best_accuracy)\n",
        "\n",
        "best_parameters = grid_search.best_params_\n",
        "print(\"The best parameters for using this model is\", best_parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfaYAhAK20mL"
      },
      "source": [
        "### Utiliza los mejores parámetros para probar de nuevo tu modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9EpQKyf20mM",
        "outputId": "4031e8ca-851a-42b1-91d7-823b7931586b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[229  12]\n",
            " [ 23 228]]\n",
            "The accuracy is 92.88617886178862 %\n",
            "The recall from the confusion matrix is 90.83665338645417 %\n",
            "[[237   4]\n",
            " [ 31 220]]\n",
            "The accuracy is 92.88617886178862 %\n",
            "The recall from the confusion matrix is 87.64940239043824 %\n"
          ]
        }
      ],
      "source": [
        "classifier_with_best_parameters =  SVC(C= best_parameters[\"C\"], \n",
        "                                       kernel= best_parameters[\"kernel\"], \n",
        "                                       random_state= 0)\n",
        "classifier_with_best_parameters.fit(X_train, y_train.ravel())\n",
        "\n",
        "#predicting the Class \n",
        "y_pred_best_parameters = classifier_with_best_parameters.predict(X_test)\n",
        "\n",
        "CM2 = confusion_matrix(y_test, y_pred_best_parameters)\n",
        "#visualizing the confusion matrix\n",
        "print(CM2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wgbaCD020mN"
      },
      "source": [
        "### ⭕ Prueba el modelo con el data set completo (sesgado)\n",
        "\n",
        "Entrena un clasificador usando el dataset original sesgado, sin usar undersampling ni oversampling.\n",
        "\n",
        "Puedes usar GridSearch (puede ser tardado). Al final, reportar las métricas de rendimiento y la matriz de confusión."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c31eW8Dh20mN"
      },
      "outputs": [],
      "source": [
        "#creating a new dataset to test our model\n",
        "datanew = credito.copy()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c2hBanv20mO"
      },
      "source": [
        "___"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}