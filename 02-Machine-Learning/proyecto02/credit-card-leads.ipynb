{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0043ec7-fa79-4e13-aeb2-8305dc349d17",
   "metadata": {},
   "source": [
    "# 02 Machine Learning\n",
    "\n",
    "## Proyecto: Credit Card Leads\n",
    "\n",
    "### Equipo:\n",
    "- Javier De La Rosa Mondragon\n",
    "- Luis Fernando Merino Nambo\n",
    "- Salomon Ramírez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c55791-20cf-4398-a3f6-b3610550c7f0",
   "metadata": {},
   "source": [
    "![Leads](credit-cards.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530a2a19-9974-4ba2-b4ad-5efdb9e18609",
   "metadata": {},
   "source": [
    "###  Dataset\n",
    "\n",
    "Credit Card Lead Prediction\n",
    "\n",
    "https://www.kaggle.com/datasets/shelvigarg/credit-card-buyers\n",
    "\n",
    "- ~250,000 datos en train set\n",
    "\n",
    "- ~100,000 datos en test set (sin eqtiquetas)\n",
    "\n",
    "- 10 Features\n",
    "    - ID\n",
    "    - Sexo\n",
    "    - Edad\n",
    "    - Región\n",
    "    - Ocupación\n",
    "    - Channel code (medio de comunicación para ofrecer la tarjeta)\n",
    "    - Vintage (meses de historial crediticio)\n",
    "    - Credit product (el cliente cuenta ya con un producto de crédito)\n",
    "    - Balance promedio en cuenta\n",
    "    - Es cleinte activo\n",
    "    \n",
    "\n",
    "\n",
    "- 1 Columna de target\n",
    "    - Lead (aceptó la tarjeta de crédito ofecida)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb5d9af-ec0d-4673-bf88-f17c974583be",
   "metadata": {},
   "source": [
    "## Objetivo\n",
    "\n",
    "Analizar, limpiar y usar el train dataset para identificar clientes con tendencia a adquirur una tarjeta de crédito tomando como features las características del cliente.\n",
    "\n",
    "Explorar las pistas que nos dan los diferentes modelos y tecnicas de clasificación para entender más cómo se pueden aplicar en diferentes casos y diferentes ámbitos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096c3d2e",
   "metadata": {},
   "source": [
    "## Retos\n",
    "\n",
    "- El dataset que utlilzaremos es muy grande, lo cual requiere que encontremos soluciones para trabajar con él y el poder de cómputo requerido puede ser poco accesible para trabajarlo dentro de rangos de tiempo compatibles con otras actividades.\n",
    "- De inmediato se puede observar que casi todas nuestras variables son categóricas. Para poder utlizarlas será necesario manipular los datos y buscar hiperparámetros adecuados para ellas (valores, distribuciones de probabilidad, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325c0461",
   "metadata": {},
   "source": [
    "## Código"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e16a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES PARA EJECUCIÓN\n",
    "DATA_FRACTION = .1 # parte del dataset completo de datos a utlizar\n",
    "LONG_TIME_CELLS = True # ejecutar las celdas de EDA que tardan mucho\n",
    "SMOTE_FRACTION = .05 # parte del dataset sobremuestreado a utlizar\n",
    "DATA_TO_USE = 'over' # 'over' | 'sub' | 'orig'  # los datos que se usarán en el entrenamiento final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46328ec-9d99-4060-9117-9e480acf2895",
   "metadata": {},
   "source": [
    "### Importación de bibliotecas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594bb085-dace-48bd-94fc-16f87a491b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical y Dfs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ML:\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2 # selección de features categoricas\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# SMOTE oversamplig\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# árboles de decisión\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# visualización de árboles con dtreeviz\n",
    "from sklearn import tree\n",
    "#from dtreeviz import model\n",
    "from dtreeviz.trees import *\n",
    "\n",
    "# visualización de árboles con graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "from six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "\n",
    "# métricas de rendimiento\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# visualización\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# timing operations\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ed54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paletas para seaborn\n",
    "palQualitative = ['#fbb4ae','#b3cde3','#ccebc5','#decbe4','#fed9a6','#ffffcc','#e5d8bd','#fddaec','#f2f2f2']\n",
    "palPaired = sns.color_palette(\"Accent\")\n",
    "sns.set()\n",
    "sns.set_palette( palQualitative )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e44afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTime(msg='time_now:',hours=1,mins=53):\n",
    "    '''imprime la hora corregida porque el servidor la tiene mal'''\n",
    "    current_time = time.localtime()\n",
    "    new_time = current_time.tm_min - mins\n",
    "    new_time = new_time % 60\n",
    "    print(msg, f\" {current_time.tm_hour-hours:02d}:{new_time:02d}:{current_time.tm_sec:02d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e74b820-e3ff-4bbc-90cf-29d86f55e888",
   "metadata": {},
   "source": [
    "### Lectura de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9658db86-58da-4d22-9b80-8a0a4057ed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/train data credit card.csv')\n",
    "print('Full Data size:',data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3964f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data (como no tenemos la etiquetas, evaluaremos nuestros modelos con subsets para test)\n",
    "TESTDATA = pd.read_csv('./data/test data credit card.csv')\n",
    "TESTDATA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d15ffc-79dc-4d31-95cc-554c45170bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe606a89-5eec-4efb-a8a1-43e517340863",
   "metadata": {},
   "source": [
    "### Limpieza de datos y EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0482e06-21b9-4d23-bfa1-765b5cc1861f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.isna().sum())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6399909-7d7b-48a0-9829-9a3ebb9f7626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limpieza de datos del set de entrenamiento\n",
    "# ya que el dataset es muy grande, empezaremos eliminando las filas con datos vaciós\n",
    "data = data.dropna()\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "display(data.isna().sum().sum())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ae1d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_piechart_with_values(data, title, ax, optional_names=None):\n",
    "    df = data\n",
    "    value_counts = df.value_counts()\n",
    "    category_counts = df.value_counts().values\n",
    "    patches, texts, autotexts = ax.pie(value_counts, labels=value_counts.index, autopct='%1.1f%%')\n",
    "\n",
    "    total = sum(value_counts.values)\n",
    "    label_values = []\n",
    "    if optional_names:\n",
    "        value_counts.index = optional_names\n",
    "        \n",
    "    for n,v in zip(value_counts.index,value_counts.values):\n",
    "        v = f'{v:,}'\n",
    "        #label_values.append(    f'{n}: {v} ({(v / total) * 100:.1f}%)'    )\n",
    "        label_values.append(    f'{n}: {v}'    )\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        text.set_text(label_values[i])\n",
    "    \n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560259b3",
   "metadata": {},
   "source": [
    "#### Variables Categóricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca0d891-24c9-40e0-9471-677adab31f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ya que trabajaremos con clasificación, veamos cuántos casos hay de cada categoría:\n",
    "\n",
    "fig, ax2 = plt.subplots(1, 1, figsize=(8, 5))\n",
    "#sns.countplot(x=data.Is_Lead, ax=ax1)\n",
    "#ax1.set_title('Countplot - Es Lead')\n",
    "#for p in ax1.patches:\n",
    "#    ax1.annotate(format(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "#                ha = 'center', va = 'center', xytext = (0, 5), textcoords = 'offset points')\n",
    "\n",
    "create_piechart_with_values(data.Is_Lead, 'Piechart - Es Lead',ax2,['No Lead','Lead'])    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064db0a4",
   "metadata": {},
   "source": [
    "**Observaciones:**\n",
    "    \n",
    "Existe una desproporción muy grande entre las dos clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4fcc50-e7f9-4484-b08d-67ef3111a2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos las distribuciones de algunas variables:\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 16))\n",
    "\n",
    "#sns.countplot(data=creditCardLeadDf, x='Gender', ax=axes[0])\n",
    "#axes[0].set_title('Sexo')\n",
    "#sns.countplot(data=creditCardLeadDf, x='Is_Active', ax=axes[0])\n",
    "#axes[0].set_title('Es cliente activo')\n",
    "#sns.countplot(data=creditCardLeadDf, x='Credit_Product', ax=axes[1])\n",
    "#axes[1].set_title('Cuenta con Producto de Crédito')\n",
    "#sns.countplot(data=creditCardLeadDf, x='Occupation', ax=axes[1])\n",
    "#axes[1].set_title('Ocupación')\n",
    "\n",
    "create_piechart_with_values(data.Gender, 'Sexo',axes[0][0])\n",
    "create_piechart_with_values(data.Occupation, 'Ocupación',axes[0][1])\n",
    "create_piechart_with_values(data.Channel_Code, 'Channel Code',axes[1][0])\n",
    "create_piechart_with_values(data.Credit_Product, 'Cuenta con Producto de Crédito',axes[1][1])\n",
    "create_piechart_with_values(data.Is_Active, 'Es cliente activo',axes[2][0])\n",
    "create_piechart_with_values(data.Is_Lead, 'Es Lead',axes[2][1])\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b61bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos las proporciones en algunas variables:\n",
    "print('Hay',data.Region_Code.value_counts().shape[0],'valores distintos de Region_Code')\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(12, 4))\n",
    "sns.countplot(data=data.sort_values(by='Region_Code'), x='Region_Code', ax=axes, order = data['Region_Code'].value_counts().index)\n",
    "axes.set_xticklabels(axes.get_xticklabels(), rotation=90)\n",
    "axes.set_title('Código de Región')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b07f847",
   "metadata": {},
   "source": [
    "**Observaciones:**\n",
    "\n",
    "Más adelante podremos determinar si esta variable es relevante para la clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eefca3f",
   "metadata": {},
   "source": [
    "\n",
    "#### Variables Numéricas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1e9a01",
   "metadata": {},
   "source": [
    "Veamos histogramas y boxplots para las variables numéricas que tienen nuestros datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a78d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createHist(data,titulo,nobins=None):\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 4))\n",
    "    #data = creditCardLeadDf['Avg_Account_Balance']\n",
    "    #titulo = 'Balance Promedio'\n",
    "\n",
    "    if nobins:\n",
    "        sns.histplot(data,ax=axes[0],bins= (data.max()-data.min()) )\n",
    "    else:\n",
    "        sns.histplot(data,ax=axes[0] )\n",
    "    axes[0].set_xlabel('Values')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title(titulo+' - Histograma')\n",
    "\n",
    "    sns.boxplot(x=data, ax=axes[1])\n",
    "    mean_value = np.mean(data)\n",
    "    axes[1].plot(mean_value, 0,  marker='o', color='red', label='Mean')\n",
    "    axes[1].text(mean_value+mean_value/1.5, -.3, f\"Mean: {mean_value:.2f}\", color='teal', ha='left', va='center')\n",
    "    #axes[1].set_xlabel('Values')\n",
    "    #axes[1].set_ylabel('Count')\n",
    "    axes[1].set_title(titulo+' - Boxplot')\n",
    "    #axes[1].legend()\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50c84aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set_palette( palQualitative )\n",
    "createHist(data['Avg_Account_Balance'],'Balance Promedio')\n",
    "createHist(data['Age'],'Edad',nobins=True)\n",
    "createHist(data['Vintage'],'Vintage',nobins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bfdfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ya que la columna ID no aporta nada a la clasificación, la eliminaremos\n",
    "data = data.drop('ID', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98a9bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18956858",
   "metadata": {},
   "source": [
    "#### Conversión de variables a one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9668910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# las siguientes columnas puedes ser codificadas como one-hot:\n",
    "# Gender (2 valores), Region_Code (35), Occupation (4), Channel_Code (4), Credit_Product (2), Is_Active (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e152da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codificamos en one-hot:\n",
    "toOneHot = ['Gender', 'Occupation', 'Is_Active', 'Credit_Product','Channel_Code', 'Region_Code']\n",
    "data = pd.get_dummies(data,columns=toOneHot)\n",
    "print(data.shape)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0945c086",
   "metadata": {},
   "source": [
    "#### Eliminación de variables innecesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904800f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para las variables que originalmente eran binarias, podemos eliminar una de sus\n",
    "# dos columnas provenientes de get_dummies (Gender, Is_Active, ):\n",
    "print(data.shape)\n",
    "data = data.drop(['Gender_Female','Is_Active_No','Credit_Product_No'], axis=1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a47abe7",
   "metadata": {},
   "source": [
    "#### Correlaciones entre variables y variables-salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f6624b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratemos de ver correlación entre las variables de Channel_Code y la salida\n",
    "x_vars = [ x for x in list(data) if 'Channel_Code' in x] \n",
    "graphsPerRow = len(x_vars)\n",
    "assert len(x_vars) % graphsPerRow == 0,f'usa un múltiplo de {len(x_vars)} para el número de gráficas por fila'\n",
    "for i in range(int(len(x_vars)/graphsPerRow)):\n",
    "    if LONG_TIME_CELLS:\n",
    "        graph = sns.pairplot(data, x_vars=x_vars[i*graphsPerRow:i*graphsPerRow+graphsPerRow], y_vars='Is_Lead', height=5, aspect=1, kind='reg')\n",
    "        graph.fig.suptitle('Correlación Channel-Salida', y=1.08)\n",
    "    plt.show()\n",
    "\n",
    "# Observaciones:    \n",
    "# Parece que por lo menos las tres primeras sí tienen un efecto en la salida\n",
    "# Aquí podría parecer que el chanel X4 es el menos efectivo obteniendo leads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0017fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratemos de ver correlación entre las variables de Occupation y la salida\n",
    "x_vars = [ x for x in list(data) if 'Occupation' in x] \n",
    "graphsPerRow = len(x_vars)\n",
    "assert len(x_vars) % graphsPerRow == 0,f'usa un múltiplo de {len(x_vars)} para el número de gráficas por fila'\n",
    "for i in range(int(len(x_vars)/graphsPerRow)):\n",
    "    if LONG_TIME_CELLS:\n",
    "        graph = sns.pairplot(data, x_vars=x_vars[i*graphsPerRow:i*graphsPerRow+graphsPerRow], y_vars='Is_Lead', height=5, aspect=1, kind='reg')\n",
    "        graph.fig.suptitle('Correlación Ocupación-Salida', y=1.08)\n",
    "    plt.show()\n",
    "\n",
    "# Observaciones:    \n",
    "# Vemos que de entre las 4, Occupation_Entrepreneur es la que tiene más efecto sobre la salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb78a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratemos de ver correlación entre nuestras variables numéricas y la salida:\n",
    "x_vars = ['Age','Avg_Account_Balance','Vintage'] \n",
    "graphsPerRow = len(x_vars)\n",
    "assert len(x_vars) % graphsPerRow == 0,f'usa un múltiplo de {len(x_vars)} para el número de gráficas por fila'\n",
    "for i in range(int(len(x_vars)/graphsPerRow)):\n",
    "    if LONG_TIME_CELLS:\n",
    "        graph = sns.pairplot(data, x_vars=x_vars[i*graphsPerRow:i*graphsPerRow+graphsPerRow], y_vars='Is_Lead', height=5, aspect=1, kind='reg')\n",
    "        graph.fig.suptitle('Correlación Variables Numéricas-Salida', y=1.08)\n",
    "    plt.show()\n",
    "    \n",
    "# Observaciones:\n",
    "# Vemos que de las tres tienen cierto grado apreciable de correlación con la salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dea677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y ahora veamos la correclación entre esas tres variables\n",
    "#sns.pairplot(data, vars=x_vars, hue='Is_Lead', markers = ['d','v'], kind = \"hist\", diag_kind = None)\n",
    "if LONG_TIME_CELLS:\n",
    "    sns.pairplot(data, vars=x_vars, hue='Is_Lead', kind = \"hist\", diag_kind = 'kde')\n",
    "\n",
    "# Observaciones:\n",
    "# No encotramos mucha correlación entre éstas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4695b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# También vemos la correlación entre estas variables usando Spearman\n",
    "display(data[x_vars].corr(method=\"spearman\"))\n",
    "\n",
    "# Observaciones:\n",
    "# Se observa una correlación significativa entre Age y Vintage, lo cual es de esperarse porque\n",
    "# sólo la gente de mayor edad puede tener más historial crediticio\n",
    "# (Yo esperaba encontrar más correlación entre edad y balance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9d2755",
   "metadata": {},
   "source": [
    "**Observaciones:**\n",
    "    \n",
    "Age y Vintage tienen correlación moderada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0381d449",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Tratemos de ver correlaciones entre las variables de región y la salida\n",
    "x_vars = list(data)\n",
    "x_vars = [ x for x in x_vars if 'Region' in x] \n",
    "\n",
    "#print(x_vars)\n",
    "#print(len(x_vars))\n",
    "\n",
    "graphsPerRow = 5\n",
    "\n",
    "assert len(x_vars) % graphsPerRow == 0,f'usa un múltiplo de {len(x_vars)} para el número de gráficas por fila'\n",
    "\n",
    "if LONG_TIME_CELLS:\n",
    "    for i in range(int(len(x_vars)/graphsPerRow)):\n",
    "        sns.pairplot(data, x_vars=x_vars[i*graphsPerRow:i*graphsPerRow+graphsPerRow], y_vars='Is_Lead', height=5, aspect=1, kind='reg')\n",
    "        plt.show()\n",
    "\n",
    "# Observaciones:\n",
    "# No notamos que alguna región destaque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98315c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos si hay otras correlaciones entre nuestras variables:\n",
    "thres = .4\n",
    "corrs = data.corr(method=\"spearman\")\n",
    "print(f'Spearman > {thres}:')\n",
    "print(f'-------------------------')\n",
    "moreCorr = []\n",
    "for i, row in corrs.iterrows():\n",
    "    for ic,val in zip(list(data),row):\n",
    "        if val>thres and i!=ic:\n",
    "            if val>=.4 and val<.69:\n",
    "                note = 'Correlación moderada'\n",
    "            elif val>.7 and val<.89:\n",
    "                note = 'Correlación fuerte'\n",
    "            elif val>=.9:\n",
    "                note = 'Correlación muy fuerte'\n",
    "                \n",
    "            variablesCorrelacionadas = [i,ic]\n",
    "            variablesCorrelacionadas.sort()\n",
    "            text = ' - '.join(variablesCorrelacionadas)\n",
    "            moreCorr.append(f' {text} : {str(round(val,3))} ({note})')\n",
    "\n",
    "moreCorr=list(set(moreCorr))\n",
    "moreCorr.sort()\n",
    "for i in moreCorr:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1588cdea",
   "metadata": {},
   "source": [
    "**Observaciones:**\n",
    "\n",
    "Se nota una correlación moderada entre Channel_Code_X3 y  Vintage (pudo ser que ese canal se enfocó en ciertos clientes con mucha o poca antiguedad).\n",
    "\n",
    "También moderada de ese mismo canal y la edad de los clientes.\n",
    "\n",
    "Y correlaciones un poco más fuertes entre el canal X1 y clientes asalariados.\n",
    "\n",
    "Desde luego, la correlación que ya habiamos notado entre edad y vintage (historial crediticio)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99f5471",
   "metadata": {},
   "source": [
    "#### Preparación de datos para los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e62cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFull = data.copy()\n",
    "X_full = dataFull.drop('Is_Lead',axis=1)\n",
    "y_full = dataFull['Is_Lead']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0490065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar desde aquí para diferentes pruebas con fracciones de los datos originales\n",
    "data = dataFull.sample(frac=DATA_FRACTION)\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "print(\"Fracción de datos utlizados para entrenamiento:\", DATA_FRACTION*100,'%')\n",
    "print('Train Data size:',data.shape)\n",
    "print('Full Data size:',dataFull.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4b5964",
   "metadata": {},
   "source": [
    "#### Primer splitting para entrenar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b898e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Is_Lead',axis=1)\n",
    "y = data['Is_Lead']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a195a3c-0186-4bdc-b9a6-4abd911881af",
   "metadata": {},
   "source": [
    "### Exploración de selección de features con SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2275889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de Features con Threshold fue descartada debido al alto número de variables categpóicas\n",
    "best=10\n",
    "selector = SelectKBest(k=best,score_func=chi2)\n",
    "\n",
    "#scaler = MinMaxScaler()\n",
    "#XScaled = scaler.fit_transform(X)\n",
    "#XScaled = pd.DataFrame(data=XScaled, columns=list(X))\n",
    "\n",
    "selector.fit(X,y)\n",
    "print(f'Las {best} mejores features son:')\n",
    "for col in selector.get_feature_names_out():\n",
    "    print(f'   {col}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deec08e",
   "metadata": {},
   "source": [
    "**Observaciones:**\n",
    "\n",
    "Las 3 variables numéricas son las más importantes de acuerdo al selector. También confirmamos que de las variables categóricas, Occupation_Entrepreneur juega el papel más importante importante."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a8d01",
   "metadata": {},
   "source": [
    "### Clasificación con árboles de decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e95ca2b-1b3c-4eb2-8b71-157495399ae2",
   "metadata": {},
   "source": [
    "#### Arboles de decisión con datos originales (1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aba81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe para guardar performace metrics\n",
    "treesPerformances = pd.DataFrame(columns=['Data', 'Acc','Recall','Precision', 'F1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0eb4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101) # 70% training and 30% test\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6816579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardemos X_test y_test originales para usarlos en los dos árboles finales:\n",
    "X_test_original = X_test.copy()\n",
    "y_test_original = y_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6953ae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances_df = pd.DataFrame({'feature':list(X),'importancia':np.round(clf.feature_importances_,3)})\n",
    "importances_df.sort_values(by='importancia',ascending=False,inplace=True)\n",
    "display(importances_df.set_index('feature').head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99de73e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pmAccuracy=round(accuracy_score(y_test_original,y_pred),3)\n",
    "pmRecall=round(recall_score(y_test_original,y_pred),3)\n",
    "pmPrecision=round(precision_score(y_test_original,y_pred),3)\n",
    "pmF1=round(f1_score(y_test, y_pred),3)\n",
    "treesPerformances.loc[len(treesPerformances.index)] = ['Datos originales', pmAccuracy, pmRecall, pmPrecision, pmF1 ] \n",
    "print(f\"Accuracy: {pmAccuracy}\")\n",
    "print(f\"Recall: {pmRecall}\")\n",
    "print(f\"Precision: {pmPrecision}\")\n",
    "\n",
    "target_labels = ['No Lead','Lead']\n",
    "plt.figure(figsize=(3,3))\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "ax = sns.heatmap(cm,cmap='plasma',annot=True, fmt='g',\n",
    "            xticklabels=target_labels,\n",
    "            yticklabels=target_labels)\n",
    "ax.set(xlabel='Predicted',ylabel='Real')\n",
    "for t in ax.texts: t.set_text('{:,d}'.format(int(float(t.get_text()))))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9087d30",
   "metadata": {},
   "source": [
    "**Observaciones:**\n",
    "    \n",
    "Al parecer el hecho de que los ejemplos están desbalanceados affecta el desempeño del modelo. A continuación balancearemos los ejemplos por medio de sub/over sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33feaa0d",
   "metadata": {},
   "source": [
    "\n",
    "#### Árboles de decisión con datos submuestreados  (1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "\n",
    "#Submuestreo\n",
    "\n",
    "# lista los índices de Is_Lead\n",
    "lead_idxs = data[data[\"Is_Lead\"]==1].index.to_list()\n",
    "print('Is_Lead == 1: ',len(lead_idxs))\n",
    "\n",
    "# lista de índices que no son lead\n",
    "nolead_idxs = data[data[\"Is_Lead\"]==0].index.to_list()\n",
    "print('Is_Lead == 0: ',len(nolead_idxs))\n",
    "\n",
    "# seleccionamos aleatoriamente el doble de índices de no leads\n",
    "random_nolead_idxs = np.random.choice(nolead_idxs, int(len(lead_idxs)*1.5), replace= False)\n",
    "print('random picks len:',len(random_nolead_idxs))\n",
    "\n",
    "# concatenamos los índices fraudulentos y normales y creamos el dataframe sub-sampleado\n",
    "undersampled_indices = np.concatenate([lead_idxs, random_nolead_idxs])\n",
    "undersampled_data = data.iloc[undersampled_indices,:]\n",
    "\n",
    "print(f\"Lead: {len(lead_idxs)}, Normales: {len(random_nolead_idxs)}\")\n",
    "\n",
    "print(f'Tamaño de data sin resampling: {data.shape}')\n",
    "print(f'Tamaño de data luego de resampling: {undersampled_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d3c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "create_piechart_with_values(data.Is_Lead, 'Datos Originales',ax1,['No','Sí'])    \n",
    "create_piechart_with_values(undersampled_data.Is_Lead, 'Datos Submuestreados',ax2,['No','Sí'])    \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd2e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"backup de data\"\n",
    "dataBkup = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae7c8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = undersampled_data.drop('Is_Lead',axis=1)\n",
    "y = undersampled_data['Is_Lead']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101) # 70% training and 30% test\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "importances_df = pd.DataFrame({'feature':list(X),'importancia':np.round(clf.feature_importances_,3)})\n",
    "importances_df.sort_values(by='importancia',ascending=False,inplace=True)\n",
    "display(importances_df.set_index('feature').head(10))\n",
    "\n",
    "pmAccuracy=round(accuracy_score(y_test,y_pred),3)\n",
    "pmRecall=round(recall_score(y_test,y_pred),3)\n",
    "pmPrecision=round(precision_score(y_test,y_pred),3)\n",
    "pmF1=round(f1_score(y_test, y_pred),3)\n",
    "# no guardamos en df para metricas (haremos otro test con los datos originales, no los submuestreados)\n",
    "print(f\"Accuracy: {pmAccuracy}\")\n",
    "print(f\"Recall: {pmRecall}\")\n",
    "print(f\"Precision: {pmPrecision}\")\n",
    "\n",
    "target_labels = ['No Lead','Lead']\n",
    "plt.figure(figsize=(3,3))\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "ax = sns.heatmap(cm,cmap='plasma',annot=True, fmt='g',\n",
    "            xticklabels=target_labels,\n",
    "            yticklabels=target_labels)\n",
    "ax.set(xlabel='Predicted',ylabel='Real')\n",
    "for t in ax.texts: t.set_text('{:,d}'.format(int(float(t.get_text()))))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fff5997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test con los datos de prueba originales:\n",
    "y_pred = clf.predict(X_test_original)\n",
    "\n",
    "importances_df = pd.DataFrame({'feature':list(X),'importancia':np.round(clf.feature_importances_,3)})\n",
    "importances_df.sort_values(by='importancia',ascending=False,inplace=True)\n",
    "display(importances_df.set_index('feature').head(10))\n",
    "\n",
    "pmAccuracy=round(accuracy_score(y_test_original,y_pred),3)\n",
    "pmRecall=round(recall_score(y_test_original,y_pred),3)\n",
    "pmPrecision=round(precision_score(y_test_original,y_pred),3)\n",
    "pmF1=round(f1_score(y_test_original, y_pred),3)\n",
    "treesPerformances.loc[len(treesPerformances.index)] = ['Sub-muestreo', pmAccuracy, pmRecall, pmPrecision, pmF1 ] \n",
    "print(f\"Accuracy: {pmAccuracy}\")\n",
    "print(f\"Recall: {pmRecall}\")\n",
    "print(f\"Precision: {pmPrecision}\")\n",
    "\n",
    "target_labels = ['No Lead','Lead']\n",
    "plt.figure(figsize=(3,3))\n",
    "cm = confusion_matrix(y_test_original,y_pred)\n",
    "ax = sns.heatmap(cm,cmap='plasma',annot=True, fmt='g',\n",
    "            xticklabels=target_labels,\n",
    "            yticklabels=target_labels)\n",
    "ax.set(xlabel='Predicted',ylabel='Real')\n",
    "for t in ax.texts: t.set_text('{:,d}'.format(int(float(t.get_text()))))\n",
    "plt.title('Matriz de confusión usando los datos originales (sin resampling)')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb914f6",
   "metadata": {},
   "source": [
    "#### Árboles de decisión con oversampling  (3/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baebc0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restauramos data\n",
    "data=dataBkup.copy()\n",
    "\n",
    "\n",
    "\n",
    "print('Datos de entrada a SMOTE:',data.shape)\n",
    "\n",
    "oversample = SMOTE()\n",
    "\n",
    "X = data.drop('Is_Lead',axis=1)\n",
    "y = data['Is_Lead']\n",
    "X_oversampled, y_oversampled = oversample.fit_resample(X, y)\n",
    "\n",
    "\n",
    "print(\"Datos SMOTE originales X, y:\", X.shape,y.shape)\n",
    "\n",
    "# lista los índices sampleados\n",
    "smoteSampleIdxs = X.index.to_list()\n",
    "print('smoteSampleIdx:',len(smoteSampleIdxs))\n",
    "randomSmoteSampleIdxs = np.random.choice(smoteSampleIdxs, int(len(smoteSampleIdxs)*SMOTE_FRACTION), replace= False)\n",
    "X_oversampled = X.iloc[randomSmoteSampleIdxs,:]\n",
    "y_oversampled = y.iloc[randomSmoteSampleIdxs]\n",
    "\n",
    "print(\"Fracción de datos SMOTE utlizados:\", SMOTE_FRACTION*100,'%')\n",
    "print(f\"Matriz de features: {X_oversampled.shape}\")\n",
    "print(f\"Matriz de etiquetas: {y_oversampled.shape}\")\n",
    "\n",
    "from collections import Counter\n",
    "print(Counter(y_oversampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe3d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_oversampled, y_oversampled, test_size=0.3) # 70% training and 30% test\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "importances_df = pd.DataFrame({'feature':list(X),'importancia':np.round(clf.feature_importances_,3)})\n",
    "importances_df.sort_values(by='importancia',ascending=False,inplace=True)\n",
    "display(importances_df.set_index('feature').head(10))\n",
    "\n",
    "pmAccuracy=round(accuracy_score(y_test,y_pred),3)\n",
    "pmRecall=round(recall_score(y_test,y_pred),3)\n",
    "pmPrecision=round(precision_score(y_test,y_pred),3)\n",
    "pmF1=round(f1_score(y_test, y_pred),3)\n",
    "# no guardamos en df de metricas, más adelante probaremos con los datos originales, no los sobremuestreados\n",
    "print(f\"Accuracy: {pmAccuracy}\")\n",
    "print(f\"Recall: {pmRecall}\")\n",
    "print(f\"Precision: {pmPrecision}\")\n",
    "\n",
    "target_labels = ['No Lead','Lead']\n",
    "plt.figure(figsize=(3,3))\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "ax = sns.heatmap(cm,cmap='plasma',annot=True, fmt='g',\n",
    "            xticklabels=target_labels,\n",
    "            yticklabels=target_labels)\n",
    "ax.set(xlabel='Predicted',ylabel='Real')\n",
    "for t in ax.texts: t.set_text('{:,d}'.format(int(float(t.get_text()))))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2bf633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# falta visualización\n",
    "viz = model(clf,\n",
    "        tree_index=3,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        feature_names=list(X_train),\n",
    "        target_name='Lead',\n",
    "        class_names=['Lead','No Lead']\n",
    ")\n",
    "\n",
    "if LONG_TIME_CELLS:\n",
    "    # no funciona\n",
    "    # viz.view(scale=1.2,fontname='DejaVu Sans')\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8f6bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_representation = tree.export_text(decision_tree=clf)\n",
    "print(X_train.columns[9])\n",
    "print(text_representation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368fe966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# falta visualización\n",
    "dot_data = StringIO()\n",
    "export_graphviz(clf, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names = list(X),class_names=['No Lead','Lead'])\n",
    "#graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "#graph.write_png('cardleads-tree.png')\n",
    "#Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f1a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test con los datos de prueba originales:\n",
    "y_pred = clf.predict(X_test_original)\n",
    "\n",
    "importances_df = pd.DataFrame({'feature':list(X),'importancia':np.round(clf.feature_importances_,3)})\n",
    "importances_df.sort_values(by='importancia',ascending=False,inplace=True)\n",
    "display(importances_df.set_index('feature').head(10))\n",
    "\n",
    "pmAccuracy=round(accuracy_score(y_test_original,y_pred),3)\n",
    "pmRecall=round(recall_score(y_test_original,y_pred),3)\n",
    "pmPrecision=round(precision_score(y_test_original,y_pred),3)\n",
    "pmF1=round(f1_score(y_test_original, y_pred),3)\n",
    "treesPerformances.loc[len(treesPerformances.index)] = ['Sobre-muestreo', pmAccuracy, pmRecall, pmPrecision, pmF1 ] \n",
    "print(f\"Accuracy: {pmAccuracy}\")\n",
    "print(f\"Recall: {pmRecall}\")\n",
    "print(f\"Precision: {pmPrecision}\")\n",
    "\n",
    "target_labels = ['No Lead','Lead']\n",
    "plt.figure(figsize=(3,3))\n",
    "cm = confusion_matrix(y_test_original,y_pred)\n",
    "ax = sns.heatmap(cm,cmap='plasma',annot=True, fmt='g',\n",
    "            xticklabels=target_labels,\n",
    "            yticklabels=target_labels)\n",
    "ax.set(xlabel='Predicted',ylabel='Real')\n",
    "for t in ax.texts: t.set_text('{:,d}'.format(int(float(t.get_text()))))\n",
    "plt.title('Matriz de confusión usando los datos originales (sin resampling)')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f81ca",
   "metadata": {},
   "source": [
    "#### Comparación de métricas de rendimiento para los árboles de decisión usando modelos entrenados con diferentes sets de datos, probados todos con los datos originales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac550b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(treesPerformances.sort_values(by='Acc', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2787b6",
   "metadata": {},
   "source": [
    "### Exploración de componentes pricipales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8931dc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# restauramos data\n",
    "data=dataBkup.copy()\n",
    "X = data.drop('Is_Lead',axis=1)\n",
    "y = data['Is_Lead']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101) # 70% training and 30% test\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=25)\n",
    "X_red = pca.fit_transform(X_scaled)\n",
    "#X_challenge_red = pca.transform(X_challenge_scaled)\n",
    "\n",
    "print('Train data antes de reducción de dimensionalidad:',X.shape)\n",
    "print('Train data luego de reducción de dimensionalidad:',X_red.shape)\n",
    "#print('Test data luego de reducción de dimensionalidad:',X_challenge_red.shape)\n",
    "\n",
    "xs = list(range(1, pca.explained_variance_ratio_.shape[0]+1))\n",
    "capturedVarThreshold = .70\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(xs, pca.explained_variance_ratio_)\n",
    "plt.xticks(xs)\n",
    "plt.plot(xs, np.cumsum(pca.explained_variance_ratio_), '-*',color='red')\n",
    "plt.grid()\n",
    "plt.xlabel(\"Componentes principales\")\n",
    "plt.ylabel(\"Varianza explicada\")\n",
    "\n",
    "captured = np.cumsum(pca.explained_variance_ratio_)\n",
    "#print(f\"Varianza explicada por cada una de las componentes principales:\\n{pca.explained_variance_ratio_}\\n\")\n",
    "#print(f\"Varianza acumulada explicada por cada una de las componentes principales:\\n{captured)}\\n\")\n",
    "\n",
    "plt.axhline(.5,color='gray',linestyle='-')\n",
    "plt.axhline(.95,color='gray',linestyle='-')\n",
    "\n",
    "maxPC = np.where(captured >= capturedVarThreshold)[0][0]\n",
    "plt.axhline(capturedVarThreshold,color='gray',linestyle='--')\n",
    "plt.axvline(maxPC,color='gray',linestyle='--')\n",
    "#plt.xticks(rotation=80)\n",
    "\n",
    "plt.title(f\"{maxPC} componentes principales capturan {capturedVarThreshold*100}% de la varianza\\n\")\n",
    "\n",
    "print(f\"El {capturedVarThreshold*100}% de la varianza se obtiene con {maxPC} componentes principales\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de3f6a6",
   "metadata": {},
   "source": [
    "### Preparación de pipeline y grid search para prueba de múltiples clasificadores con múltiples hiperparámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9d1d08",
   "metadata": {},
   "source": [
    "Usaremos los datos ~~submuestreados~~ sobremuestreados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c66b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_TO_USE == 'over':\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_oversampled, y_oversampled, test_size=0.3, random_state=101) # 70% training and 30% test\n",
    "    X = X_oversampled\n",
    "    y = y_oversampled\n",
    "elif DATA_TO_USE == 'sub':\n",
    "    X = undersampled_data.drop('Is_Lead',axis=1)\n",
    "    y = undersampled_data['Is_Lead']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101) # 70% training and 30% test\n",
    "elif DATA_TO_USE == 'orig':\n",
    "    data = dataBkup.copy()\n",
    "    X = data.drop('Is_Lead',axis=1)\n",
    "    y = data['Is_Lead']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101) # 70% training and 30% test\n",
    "\n",
    "print(f\"Usando data: {DATA_TO_USE}\")    \n",
    "print('X_train: {:,d}, X_test: {:,d}, y_train: {:,d}, y_test: {:,d}'.format(X_train.shape[0],X_test.shape[0],y_train.shape[0],y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac2c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe para guardar performace metrics\n",
    "classifiersPerformances = pd.DataFrame(columns=['Classifier','Data',\n",
    "                                                'Acc','Recall','Precision','F1','Roc-Auc',\n",
    "                                               ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c03ce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALER = True\n",
    "POLY = False\n",
    "DIMRED = True # si no hacemos dimRed el fitting toma demasiado tiempo\n",
    "PCA_COMPONENTS = maxPC \n",
    "\n",
    "# CLASSIFIER /////////////////////(editar para cambiar el clasificador)/////////////////////\n",
    "#classifier = RandomForestClassifier(n_jobs=-1)\n",
    "classifier = SVC(probability=True)\n",
    "#classifier =  RidgeClassifier()\n",
    "\n",
    "# SCALER OBJECT\n",
    "scaler = MinMaxScaler() if SCALER else None\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "# POLYNOMIAL FEATURES OBJECT\n",
    "poly = PolynomialFeatures(include_bias=True) if POLY else None\n",
    "\n",
    "# DIMENSIONALITY REDUCTION OBJECT\n",
    "dimRed = PCA(n_components=PCA_COMPONENTS) if DIMRED else None\n",
    "\n",
    "# parámetros a probar con grid search:\n",
    "\n",
    "# params for SVM:\n",
    "#if type(classifier) == type(SVC()):\n",
    "#    gridParams = [\n",
    "#        {\n",
    "#            'classifier__C': [.01,.1, 1, 10, 100, 1000],\n",
    "#            'classifier__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "#            'classifier__gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "#            'poly__degree': [1,2,3]\n",
    "#        },\n",
    "#    ]\n",
    "    \n",
    "\n",
    "# Larger C values (e.g., 1, 10, 100) can be useful when:\n",
    "#  The dataset is clean and has limited noise.\n",
    "#  You want to capture complex decision boundaries.\n",
    "#  You are willing to tolerate some misclassifications.\n",
    "\n",
    "# Smaller gamma values (e.g., 0.001, 0.01) can be useful when:\n",
    "#  The dataset is large.\n",
    "#  The decision boundary is expected to be relatively smooth.\n",
    "#  The classes are well-separated and the data is not too complex.\n",
    "\n",
    "# params for SVM:\n",
    "if type(classifier) == type(SVC()):\n",
    "    gridParams = [\n",
    "        {\n",
    "            'classifier__C': [5, 10, 50],\n",
    "            'classifier__kernel': ['poly', 'rbf', 'sigmoid'],\n",
    "            'classifier__gamma': [.001,0.1, 0.5],\n",
    "            'poly__degree': [1,3]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "# params for RandomForest:\n",
    "if type(classifier) == type(RandomForestClassifier(n_jobs=-1)):\n",
    "    gridParams = [\n",
    "        {\n",
    "            'classifier__n_estimators':[10,50,100]\n",
    "        }]\n",
    "    \n",
    "# params for Linear(Ridge):\n",
    "if type(classifier) == type(RidgeClassifier()):\n",
    "    gridParams = [{\n",
    "        'classifier__alpha':[.01,.1, 1,5,10],\n",
    "    }]\n",
    "\n",
    "# quitamos los parámetros que no se utilizarán\n",
    "newParams = []\n",
    "for d in gridParams:\n",
    "    td = d.copy()\n",
    "    for k in d.keys():\n",
    "        if 'scaler' in k and not SCALER: del(td[k])\n",
    "        if 'poly' in k and not POLY: del(td[k])\n",
    "        if 'dimRed' in k and not DIMRED: del(td[k])\n",
    "    newParams.append(td)\n",
    "gridParams = newParams\n",
    "            \n",
    "# definimos los pasos del pipeline\n",
    "steps=[\n",
    "    ('scaler',scaler if scaler else 'passthrough'),\n",
    "    ('dimRed', dimRed if dimRed else 'passthrough'),\n",
    "    ('poly',poly if poly else 'passthrough'),\n",
    "    ('classifier',classifier)\n",
    "]\n",
    "\n",
    "# quitamos del pipeline los pasos que no utilizaremos\n",
    "steps = [s for s in steps if s[1]!='passthrough']\n",
    "pipe = Pipeline(steps=steps)\n",
    "\n",
    "# instanciamos un objeto de GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator = pipe,\n",
    "    param_grid = gridParams,\n",
    "    scoring = 'accuracy',\n",
    "    cv = 5, # incluimos cross validations en el grid search\n",
    "    n_jobs = -1,\n",
    ")\n",
    "\n",
    "print('Pasos del pipeline:')\n",
    "for s in [str(step) for step in pipe.steps]: print('\\t',s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702299ea",
   "metadata": {},
   "source": [
    "### Búsqueda de hiperparámetros y entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f58c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model and time it\n",
    "start_time = time.time()\n",
    "#print('inicio:',time.asctime())\n",
    "printTime('inicio:')\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "#print('fin:',time.asctime())\n",
    "printTime('fin:')\n",
    "print('Tiempo para fit operations:',round((duration)/60,2),'minutos')\n",
    "print('Datos usados:',DATA_TO_USE)\n",
    "print('X_train: {:,d}, X_test: {:,d}, y_train: {:,d}, y_test: {:,d}'.format(X_train.shape[0],X_test.shape[0],y_train.shape[0],y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f4caf8",
   "metadata": {},
   "source": [
    "### Mejor estimador y métricas de rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5828d630",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_train)\n",
    "\n",
    "best_accuracy = grid_search.best_score_\n",
    "best_parameters = grid_search.best_params_\n",
    "\n",
    "print(\"Fracción de datos utlizados para set inicial de entrenamiento:\", DATA_FRACTION*100,'%')\n",
    "print()\n",
    "print('Datos usados para entrenamiento (originales, subsampled, oversampled):',DATA_TO_USE)\n",
    "print('X_train: {:,d}, X_test: {:,d}, y_train: {:,d}, y_test: {:,d}'.format(X_train.shape[0],X_test.shape[0],y_train.shape[0],y_test.shape[0]))\n",
    "print()\n",
    "print('Tiempo para fit operations:',round((duration)/60,2),'minutos')\n",
    "print()\n",
    "print(\"Componentes principales utlizados\",maxPC,f'({capturedVarThreshold*100}% de la varianza)')\n",
    "print()\n",
    "print(\"Mejor accuracy gridSearch:\", round(best_accuracy,4))\n",
    "print()\n",
    "print(\"Hiperparámetros del mejor modelo de gridsearch:\")\n",
    "print(best_parameters)\n",
    "print()\n",
    "print('Pasos del pipeline:')\n",
    "for s in [str(step) for step in pipe.steps]: print('\\t',s)\n",
    "print()\n",
    "\n",
    "# creamos un figura con tres subplots (train, test, test original)\n",
    "target_labels = ['No Lead','Lead']\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3),sharey=True)\n",
    "\n",
    "\n",
    "figR, axesR = plt.subplots(1, 3, figsize=(12, 5),sharey=True)\n",
    "\n",
    "\n",
    "# train\n",
    "y_pred = best_model.predict(X_train)\n",
    "y_pred_proba = best_model.predict_proba(X_train)[:, 1]\n",
    "pmScoreTrain=round(best_model.score(X_train,y_train),3)\n",
    "pmAccuracyTrain=round(accuracy_score(y_train,y_pred),3)\n",
    "pmRecallTrain=round(recall_score(y_train,y_pred),3)\n",
    "pmPrecisionTrain=round(precision_score(y_train,y_pred),3)\n",
    "pmF1Train=round(f1_score(y_train,y_pred),3)\n",
    "pmRocAucTrain = roc_auc_score(y_train, y_pred_proba)\n",
    "\n",
    "cm = confusion_matrix(y_train,y_pred)\n",
    "ax=axes[0]\n",
    "sns.heatmap(cm,cmap='plasma',annot=True, fmt='g',xticklabels=target_labels,yticklabels=target_labels,ax=ax)\n",
    "ax.set(xlabel='Predicted',ylabel='Real')\n",
    "for t in ax.texts: t.set_text('{:,d}'.format(int(float(t.get_text()))))\n",
    "ax.set_title('Train')\n",
    "\n",
    "ax=axesR[0]\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_pred_proba)\n",
    "#plt.figure()\n",
    "ax.set_title(\"Curva ROC (train)\") \n",
    "ax.plot(fpr,tpr,color='red')\n",
    "ax.plot([0,1],[0,1],linestyle='--',color='gray')\n",
    "ax.set_xlabel(\"FPR\")\n",
    "ax.set_ylabel(\"TPR\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# test\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "pmScoreTest=round(best_model.score(X_test,y_test),3)\n",
    "pmAccuracyTest=round(accuracy_score(y_test,y_pred),3)\n",
    "pmRecallTest=round(recall_score(y_test,y_pred),3)\n",
    "pmPrecisionTest=round(precision_score(y_test,y_pred),3)\n",
    "pmF1Test=round(f1_score(y_test,y_pred),3)\n",
    "pmRocAucTest = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "ax=axes[1]\n",
    "sns.heatmap(cm,cmap='plasma',annot=True, fmt='g',xticklabels=target_labels,yticklabels=target_labels,ax=ax)\n",
    "ax.set(xlabel='Predicted',ylabel='Real')\n",
    "for t in ax.texts: t.set_text('{:,d}'.format(int(float(t.get_text()))))\n",
    "ax.set_title('Test')\n",
    "\n",
    "ax=axesR[1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "#plt.figure()\n",
    "ax.set_title(\"Curva ROC (test)\") \n",
    "ax.plot(fpr,tpr,color='red')\n",
    "ax.plot([0,1],[0,1],linestyle='--',color='gray')\n",
    "ax.set_xlabel(\"FPR\")\n",
    "ax.set_ylabel(\"TPR\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# test con los datos de prueba originales:\n",
    "#y_pred = best_model.predict(X_test_original)\n",
    "y_pred = best_model.predict(X_full)\n",
    "y_pred_proba = best_model.predict_proba(X_full)[:, 1]\n",
    "#pmScoreTestOrig=round(best_model.score(X_test_original,y_pred),3)\n",
    "pmScoreTestOrig=round(best_model.score(X_full,y_full),3)\n",
    "#pmAccuracyTestOrig=round(accuracy_score(y_test_original,y_pred),3)\n",
    "pmAccuracyTestOrig=round(accuracy_score(y_full,y_pred),3)\n",
    "#pmRecallTestOrig=round(recall_score(y_test_original,y_pred),3)\n",
    "pmRecallTestOrig=round(recall_score(y_full,y_pred),3)\n",
    "#pmPrecisionTestOrig=round(precision_score(y_test_original,y_pred),3)\n",
    "pmPrecisionTestOrig=round(precision_score(y_full,y_pred),3)\n",
    "#pmF1TestOrig=round(f1_score(y_test_original, y_pred),3)\n",
    "pmF1TestOrig=round(f1_score(y_full, y_pred),3)\n",
    "pmRocAucTestOrig = roc_auc_score(y_full, y_pred_proba)\n",
    "\n",
    "#cm = confusion_matrix(y_test_original,y_pred)\n",
    "cm = confusion_matrix(y_full,y_pred)\n",
    "ax=axes[2]\n",
    "sns.heatmap(cm,cmap='plasma',annot=True, fmt='g',xticklabels=target_labels,yticklabels=target_labels,ax=ax)\n",
    "ax.set(xlabel='Predicted',ylabel='Real')\n",
    "for t in ax.texts: t.set_text('{:,d}'.format(int(float(t.get_text()))))\n",
    "ax.set_title('Test (dataset completo)')\n",
    "\n",
    "ax=axesR[2]\n",
    "fpr, tpr, thresholds = roc_curve(y_full, y_pred_proba)\n",
    "#plt.figure()\n",
    "ax.set_title(\"Curva ROC (dataset completo)\") \n",
    "ax.plot(fpr,tpr,color='red')\n",
    "ax.plot([0,1],[0,1],linestyle='--',color='gray')\n",
    "ax.set_xlabel(\"FPR\")\n",
    "ax.set_ylabel(\"TPR\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig.suptitle('Matrices de confusión')\n",
    "figR.suptitle('Curvas Roc-Auc')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classifiersPerformances.loc[len(classifiersPerformances.index)] = [type(pipe['classifier']),'Train',\n",
    "                                                                  pmAccuracyTrain, pmRecallTrain, pmPrecisionTrain, pmF1Train,pmRocAucTrain\n",
    "                                                                  ] \n",
    "\n",
    "classifiersPerformances.loc[len(classifiersPerformances.index)] = [type(pipe['classifier']),'Test',\n",
    "                                                                  pmAccuracyTest, pmRecallTest, pmPrecisionTest, pmF1Test,pmRocAucTest\n",
    "                                                                  ] \n",
    "\n",
    "classifiersPerformances.loc[len(classifiersPerformances.index)] = [type(pipe['classifier']),'Test Full',\n",
    "                                                                  pmAccuracyTestOrig, pmRecallTestOrig, pmPrecisionTestOrig, pmF1TestOrig,pmRocAucTestOrig\n",
    "                                                                  ] \n",
    "\n",
    "display(classifiersPerformances)\n",
    "\n",
    "if type(classifier) == type(RandomForestClassifier(n_jobs=-1)):\n",
    "    if SCALER:\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_train = X_train_scaled\n",
    "        \n",
    "    if DIMRED:\n",
    "        pca = PCA(n_components=maxPC)\n",
    "        X_train_red = pca.fit_transform(X_train)\n",
    "        X_train = X_train_red\n",
    "    \n",
    "    feature_cols = pd.DataFrame(X_train_red).columns.to_list()\n",
    "    classifier.fit(X_train,y_train)\n",
    "    \n",
    "    importances = pd.DataFrame({'feature':feature_cols,'importancia':np.round(classifier.feature_importances_,3)})\n",
    "    importances.sort_values(by='importancia',ascending=False,inplace=True)\n",
    "    importances.set_index('feature')\n",
    "    print(importances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30b9972",
   "metadata": {},
   "source": [
    "### Clustering con DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2650d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = DBSCAN(eps=150, min_samples=12)\n",
    "yhat = modelo.fit_predict(X)\n",
    "clusters = [j for j in np.unique(yhat) if j!=-1]\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8f76e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "XWithYAndCluster = pd.concat((X.reset_index(drop=True),y.reset_index(drop=True),pd.DataFrame(yhat,columns=['Cluster'])), axis=1)\n",
    "\n",
    "print(\"Clientes que no fueron agregados a un cluster:\",XWithYAndCluster[XWithYAndCluster.Cluster==-1].shape[0],'/',XWithYAndCluster.shape[0])\n",
    "\n",
    "clientesEnClustersTotales=0\n",
    "clientesEnClusters=[]\n",
    "for cluster in clusters:\n",
    "    clientesEnClusters.append( XWithYAndCluster[XWithYAndCluster.Cluster==cluster].shape[0] )\n",
    "    clientesEnClustersTotales += XWithYAndCluster[XWithYAndCluster.Cluster==cluster].shape[0]\n",
    "\n",
    "plt.figure(figsize=(12,5), dpi=300)\n",
    "sns.barplot(x=np.arange(len(clientesEnClusters)), y=clientesEnClusters)\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Clientes\")\n",
    "plt.title(\"Clientes en cada cluster\")\n",
    "plt.show()\n",
    "\n",
    "print('El total de clientes en clusters es de:',clientesEnClustersTotales)\n",
    "#display(XWithYAndCluster.head())\n",
    "print('Estadísticas descriptivas del cluster 6:')\n",
    "display(XWithYAndCluster[XWithYAndCluster.Cluster==6].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfa118b",
   "metadata": {},
   "source": [
    "**Observaciones:**\n",
    "\n",
    "Los clientes dentro de cada cluster deben tener características similares. Revisando las desviaciones estándar de cada feature podemos hacernos cierta idea de las features que comparten:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f5e587",
   "metadata": {},
   "source": [
    "#### Desviación estándar de cada feature de los datos del un par de clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7854b047",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusterNumber = 5\n",
    "stds=XWithYAndCluster[(XWithYAndCluster.Cluster==clusterNumber)]\n",
    "stds.describe().loc['std'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747eb67c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusterNumber = 9\n",
    "stds=XWithYAndCluster[(XWithYAndCluster.Cluster==clusterNumber)]\n",
    "stds.describe().loc['std'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e061b",
   "metadata": {},
   "source": [
    "### Clustering con Jerárquico usando PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e57a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove these X and y assigns (already in appropiate cell):\n",
    "#X = X_oversampled\n",
    "#y = y_oversampled\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "pca = PCA(n_components=2)\n",
    "X_red = pca.fit_transform(X_scaled)\n",
    "\n",
    "explained_variance_ratios = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratios)\n",
    "for i in range(2):\n",
    "    print(f\"Varianza explicada para el componente {i+1}: {explained_variance_ratios[i]}\")\n",
    "print(f\"Varianza acumulada para {2} componentes: {cumulative_variance[2-1]}\")\n",
    "\n",
    "modelo = AgglomerativeClustering(distance_threshold=5.0,\n",
    "                                 n_clusters=None,\n",
    "                                 compute_full_tree=True)\n",
    "\n",
    "#modelo = DBSCAN(eps=5000, min_samples=50) # no logré un clustering con DBSCAN sobre este dataset\n",
    "\n",
    "yhat = modelo.fit_predict(X_red)\n",
    "clusters = [j for j in np.unique(yhat) if j!=-1]\n",
    "print(\"Clusters:\")\n",
    "print(clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb16cc96",
   "metadata": {},
   "source": [
    "**Observaciones:**\n",
    "\n",
    "No logré un clustering sobre estos datos usando DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3784d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "XWithYAndCluster = pd.concat((X.reset_index(drop=True),y.reset_index(drop=True),pd.DataFrame(yhat,columns=['Cluster'])), axis=1)\n",
    "\n",
    "print(\"Clientes que no fueron agregados a un cluster:\",XWithYAndCluster[XWithYAndCluster.Cluster==-1].shape[0],'/',XWithYAndCluster.shape[0])\n",
    "\n",
    "clientesEnClustersTotales=0\n",
    "clientesEnClusters=[]\n",
    "for cluster in clusters:\n",
    "    clientesEnClusters.append( XWithYAndCluster[XWithYAndCluster.Cluster==cluster].shape[0] )\n",
    "    clientesEnClustersTotales += XWithYAndCluster[XWithYAndCluster.Cluster==cluster].shape[0]\n",
    "\n",
    "plt.figure(figsize=(12,5),dpi=300)\n",
    "sns.barplot(x=np.arange(len(clientesEnClusters)), y=clientesEnClusters)\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Clientes\")\n",
    "plt.title(\"Clientes en cada cluster\")\n",
    "plt.show()\n",
    "\n",
    "print('El total de clientes en clusters es de:',clientesEnClustersTotales)\n",
    "#display(XWithYAndCluster.head())\n",
    "print('Estadísticas descriptivas del cluster 6:')\n",
    "display(XWithYAndCluster[XWithYAndCluster.Cluster==6].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f703c9",
   "metadata": {},
   "source": [
    "#### Desviación estándar de cada feature de un cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1293c80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clusterNumber = 7\n",
    "stds=XWithYAndCluster[(XWithYAndCluster.Cluster==clusterNumber)]\n",
    "stds.describe().loc['std'].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40184350",
   "metadata": {},
   "source": [
    "**Observaciones:**\n",
    "\n",
    "Podríamos encontrar qué cluster se basa más en cierta feature, Vintage, por ejemplo si comparamos la desviación estándar de cada cluster y encontramos cuál cluster tiene menos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48137924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting de clusters\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 4),dpi=300)\n",
    "sns.scatterplot(x = X_red[:,1], y = X_red[:,0], hue = y, palette='deep', ax=axes[0], s=30)\n",
    "axes[0].set_title('Is_Lead')\n",
    "sns.scatterplot(x = X_red[:,1], y = X_red[:,0], hue = yhat==5, palette='deep', ax=axes[1], s=30)\n",
    "axes[1].set_title('Cluster 5')\n",
    "plt.suptitle(f'Clustering Jerárquico con 2CP ({round(cumulative_variance[-1],3)}% de la varianza)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 4),dpi=300)\n",
    "sns.scatterplot(x = X_red[:,1], y = X_red[:,0], hue = yhat==7, palette='deep', ax=axes[0], s=30)\n",
    "axes[0].set_title('Cluster 7')\n",
    "sns.scatterplot(x = X_red[:,1], y = X_red[:,0], hue = yhat==11, palette='deep', ax=axes[1], s=30)\n",
    "axes[1].set_title('Cluster 11')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 4),dpi=300)\n",
    "sns.scatterplot(x = X_red[:,1], y = X_red[:,0], hue = yhat==4, palette='deep', ax=axes[0], s=30)\n",
    "axes[0].set_title('Cluster 4')\n",
    "sns.scatterplot(x = X_red[:,1], y = X_red[:,0], hue = yhat==10, palette='deep', ax=axes[1], s=30)\n",
    "axes[1].set_title('Cluster 10')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4088a84",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "El trabajo realizado fue de inmensa utilidad para comprender más la implementación de muchos modelos y técnicas vistas en el módulo dos del diplomado.\n",
    "\n",
    "El análisis de cluster puede revelar conocimientos de los datos que en primera instancia están muy ocultos.\n",
    "\n",
    "El tamaño del dataset es un factor principal a tomar en cuenta al acercarse a un problema donde pueda aplicarse ML y en el caso del trabajo aquí presentado fue una de las carácteríticas más dificil de tratar para llegar a buenos modelos de clasificación.\n",
    "\n",
    "Es necesario consultar continuamente la documentación de las implementaciones en Python así como la teoría detrás de cada modelo y algoritmos para familiarizarse plenamente con la materia.\n",
    "\n",
    "El dataset y planteamiento del problema fue parte de un job-a-ton. La tabla de ranking de soluciones de varias pesonas se encuentra en https://datahack.analyticsvidhya.com/contest/job-a-thon-2/#LeaderBoard \n",
    "\n",
    "El máximo score alcanzado hasta ahora por la comunidad es de 0.8735155589"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
